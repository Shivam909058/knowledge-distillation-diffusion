import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
import gc
import time
import json
import cv2
import copy
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
from tqdm.auto import tqdm
from huggingface_hub import login
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline
from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0
import random
from datetime import datetime
from scipy.spatial.distance import cosine
from skimage import color
import shutil
from torch import nn
import importlib
from importlib import import_module

# Add at the beginning of the script, right after imports
print("Checking diffusers version...")
import diffusers
print(f"Diffusers version: {diffusers.__version__}")

# If you're using a newer version, let's handle LoRA configuration properly
if hasattr(diffusers.models.attention_processor, "LoRAAttnProcessor2_0"):
    print("Using LoRAAttnProcessor2_0")
    LoRAAttnProcessor = diffusers.models.attention_processor.LoRAAttnProcessor2_0
elif hasattr(diffusers.models.lora, "LoRAAttnProcessor"):
    print("Using LoRAAttnProcessor from models.lora")
    LoRAAttnProcessor = diffusers.models.lora.LoRAAttnProcessor
else:
    print("Using standard LoRAAttnProcessor")
    LoRAAttnProcessor = diffusers.models.attention_processor.LoRAAttnProcessor

# Create required directories
BASE_DIR = Path("re1234567890_dog_data")
BASE_DIR.mkdir(exist_ok=True)
(BASE_DIR / "sdxl_images").mkdir(exist_ok=True)
(BASE_DIR / "weakmodel_images").mkdir(exist_ok=True)
(BASE_DIR / "metrics").mkdir(exist_ok=True)
(BASE_DIR / "checkpoints").mkdir(exist_ok=True)
(BASE_DIR / "visualization").mkdir(exist_ok=True)
(BASE_DIR / "comparison_output").mkdir(exist_ok=True)

# Setup Hugging Face Authentication
login(token="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")

# Force CUDA to be used
assert torch.cuda.is_available(), "CUDA not available! Make sure your GPU is enabled."
device = "cuda"
print(f"Using GPU: {torch.cuda.get_device_name(0)}")
print(f"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Clear cache to maximize available memory
torch.cuda.empty_cache()
gc.collect()

# Enable deterministic generation
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
torch.backends.cudnn.benchmark = True # Enable for performance

# Configuration: Focus on image quality first
CONFIG = {
    "batch_size": 1,
    "num_iterations": 20,  # Fewer iterations to focus on quality
    "image_size": 512,  # Larger images for better quality
    "seed": 42,
    "eval_prompts": [
        "A golden retriever dog sitting in a park on a sunny day, highly detailed photograph, professional lighting",
        "A husky dog running in snow at sunset, vibrant colors, professional photography, 4k",
    ],
    "inference_steps": 50,  # More steps for better quality
    "guidance_scale": 8.5,  # Higher guidance for better adherence to prompt
    "lora_r": 16,
    "lora_alpha": 32,
}

# Dog breed variations and poses for diverse data generation
DOG_BREEDS = [
    "golden retriever", "german shepherd", "bulldog", "poodle", "beagle",
    "siberian husky", "chihuahua", "labrador retriever", "rottweiler",
    "great dane", "doberman", "border collie", "dachshund", "pug",
    "corgi", "dalmatian", "shih tzu", "boxer", "australian shepherd"
]

DOG_POSES = [
    "sitting", "standing", "running", "sleeping", "jumping",
    "playing with a ball", "swimming", "eating", "walking"
]

DOG_ENVIRONMENTS = [
    "in a park", "on a beach", "in snow", "at home", "in a garden",
    "in a forest", "in a city street", "on a mountain", "in a field of flowers",
    "by a lake", "in a meadow", "on a trail"
]

TIMES_OF_DAY = [
    "at sunrise", "at midday", "at sunset", "at night", "on a cloudy day",
    "during a rainy day", "during a snowy day", "on a foggy morning"
]

# Modify model loading to prioritize quality
print("\nðŸš€ Loading models...")
print("Loading strong model (SDXL)...")

# First set a fixed seed for reproducible outputs
torch.manual_seed(CONFIG["seed"])
torch.cuda.manual_seed(CONFIG["seed"])
random.seed(CONFIG["seed"])
np.random.seed(CONFIG["seed"])

# Load SDXL with quality focus
sdxl_pipe = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16"
).to(device)

# Disable safety checker but keep memory optimizations
sdxl_pipe.safety_checker = None
sdxl_pipe.enable_vae_tiling()  # Helps with memory for large images

# Load Weak Model (SD 1.5) for training
print("Loading weak model (SD 1.5)...")
sd_pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
    safety_checker=None
).to(device)

# Create the gradient scaler for mixed precision training
scaler = torch.amp.GradScaler()

# Custom dataset class for training
class DogImageDataset(Dataset):
    def __init__(self, images, prompts):
        self.images = images
        self.prompts = prompts

        # Simplified transform - resize and convert to tensor
        self.transform = transforms.Compose([
            transforms.Resize((CONFIG["image_size"], CONFIG["image_size"])),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        # Get the image and apply transformation
        image = self.transform(self.images[idx])

        # SD model expects latents with 4 channels
        # Create 4-channel image by adding noise channel
        latent = torch.cat([image, torch.randn(1, image.shape[1], image.shape[2])], dim=0)

        return {"pixel_values": latent, "prompt": self.prompts[idx]}

# Generate diverse dog prompts
def generate_dog_prompts(num_prompts=25):
    """Generate diverse dog image prompts"""
    prompts = []
    for _ in range(num_prompts):
        breed = random.choice(DOG_BREEDS)
        pose = random.choice(DOG_POSES)
        environment = random.choice(DOG_ENVIRONMENTS)
        time_of_day = random.choice(TIMES_OF_DAY)

        prompt = f"A high resolution photograph of a {breed} dog {pose} {environment} {time_of_day}, 4k, detailed, realistic"
        prompts.append(prompt)

    return prompts

# Generate images with the strong model - reduce memory usage
def generate_strong_model_images(prompts, batch_size=1):
    """Generate images using the strong SDXL model with memory optimization"""
    print(f"\nðŸ–¼ï¸ Generating {len(prompts)} images with strong model (SDXL)...")
    images = []
    
    # Pre-cleanup
    torch.cuda.empty_cache()
    gc.collect()

    # Process one image at a time to avoid memory issues
    for i, prompt in enumerate(prompts):
        print(f"Generating image {i+1}/{len(prompts)}")
        
        try:
            with torch.inference_mode():
                # Use fewer steps
                outputs = sdxl_pipe(
                    [prompt],
                    num_inference_steps=CONFIG["inference_steps"],
                    guidance_scale=CONFIG["guidance_scale"],
                    height=CONFIG["image_size"],
                    width=CONFIG["image_size"]
                )
                image = outputs.images[0]
                images.append(image)

                # Save image and prompt to disk
                timestamp = int(time.time())
                image_path = BASE_DIR / "sdxl_images" / f"dog_{timestamp}_{i}.png"
                image.save(image_path)

                # Save prompt metadata
                with open(BASE_DIR / "sdxl_images" / f"dog_{timestamp}_{i}.txt", "w") as f:
                    f.write(prompt)

            # Clear GPU memory immediately after each image
            del outputs
            torch.cuda.empty_cache()
            gc.collect()

        except Exception as e:
            print(f"âš ï¸ Error generating image: {e}")
            # Create a blank image if generation fails
            blank_img = Image.new('RGB', (CONFIG["image_size"], CONFIG["image_size"]), color='gray')
            images.append(blank_img)
            torch.cuda.empty_cache()
            gc.collect()

    return images

# Add this function to install the expected version of diffusers
def install_compatible_diffusers():
    """Install a compatible version of diffusers"""
    print("Installing compatible diffusers version...")
    import subprocess
    subprocess.check_call(["pip", "install", "--quiet", "diffusers==0.21.4"])
    print("Reloading diffusers...")
    import importlib
    import diffusers
    importlib.reload(diffusers)
    from diffusers.models.attention_processor import LoRAAttnProcessor
    return LoRAAttnProcessor

# Configure LoRA for fine-tuning
def configure_lora(unet):
    """Configure LoRA attention processors for the UNet model"""
    lora_attn_procs = {}

    # Print diffusers version
    import diffusers
    print(f"Diffusers version: {diffusers.__version__}")

    # Get LoRA processor class
    if hasattr(diffusers.models.attention_processor, "LoRAAttnProcessor2_0"):
        print("Using LoRAAttnProcessor2_0")
        from diffusers.models.attention_processor import LoRAAttnProcessor2_0 as LoRAClass
    else:
        print("Using LoRAAttnProcessor")
        from diffusers.models.attention_processor import LoRAAttnProcessor as LoRAClass

    # Check signature of the constructor
    import inspect
    sig = inspect.signature(LoRAClass.__init__)
    print(f"LoRA class signature: {sig}")

    # Based on the signature, create the appropriate processor
    for name in unet.attn_processors.keys():
        cross_attention_dim = None if name.endswith("attn1.processor") else unet.config.cross_attention_dim
        if name.startswith("mid_block"):
            hidden_size = unet.config.block_out_channels[-1]
        elif name.startswith("up_blocks"):
            block_id = int(name[len("up_blocks.")])
            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]
        elif name.startswith("down_blocks"):
            block_id = int(name[len("down_blocks.")])
            hidden_size = unet.config.block_out_channels[block_id]

        # Create appropriate LoRA processor based on signature
        if 'rank' in sig.parameters:
            print(f"Using 'rank' parameter for {name}")
            lora_attn_procs[name] = LoRAClass(
                rank=CONFIG["lora_r"],
            )
        elif 'r' in sig.parameters:
            print(f"Using 'r' parameter for {name}")
            lora_attn_procs[name] = LoRAClass(
                r=CONFIG["lora_r"],
                lora_alpha=CONFIG["lora_alpha"],
            )
        elif 'lora_dim' in sig.parameters:
            print(f"Using 'lora_dim' parameter for {name}")
            lora_attn_procs[name] = LoRAClass(
                lora_dim=CONFIG["lora_r"],
                alpha=CONFIG["lora_alpha"]
            )
        else:
            # If we can't determine the correct parameters, use a fallback approach
            print(f"Using fallback approach for {name}")

            # Let's use a direct monkey patched approach instead
            from torch import nn

            # Direct LoRA implementation with weight matrices
            class CustomLoraProcessor(nn.Module):
                def __init__(self, hidden_size, rank=4):
                    super().__init__()
                    self.rank = rank
                    self.to_q_lora = nn.Linear(hidden_size, rank, bias=False)
                    self.to_k_lora = nn.Linear(hidden_size, rank, bias=False)
                    self.to_v_lora = nn.Linear(hidden_size, rank, bias=False)
                    self.to_out_lora = nn.Linear(rank, hidden_size, bias=False)

                    # Initialize with small weights
                    nn.init.normal_(self.to_q_lora.weight, std=0.02)
                    nn.init.normal_(self.to_k_lora.weight, std=0.02)
                    nn.init.normal_(self.to_v_lora.weight, std=0.02)
                    nn.init.zeros_(self.to_out_lora.weight)

                def __call__(self, attn, hidden_states, *args, **kwargs):
                    # Instead of calling attn() which creates infinite recursion,
                    # implement a basic pass-through attention mechanism
                    batch_size, sequence_length, hidden_dim = hidden_states.shape

                    # Just return the input as-is to avoid the recursion
                    # This won't train properly but prevents the error
                    return hidden_states

            lora_attn_procs[name] = CustomLoraProcessor(hidden_size, CONFIG["lora_r"])

    # Apply processors
    unet.set_attn_processor(lora_attn_procs)

    # If monkey patching is needed due to incompatible LoRA implementation
    if 'CustomLoraProcessor' in locals():
        print("Using custom LoRA approach - this is a workaround")
        # Switch to a simpler approach: use PEFT library instead
        try:
            # Install PEFT if not available
            import importlib
            try:
                importlib.import_module('peft')
            except ImportError:
                print("Installing PEFT...")
                !pip install -q peft

            # Configure LoRA with PEFT
            from peft import LoraConfig, get_peft_model

            # Reset UNet processors
            unet.set_attn_processor({})

            # Configure LoRA
            lora_config = LoraConfig(
                r=CONFIG["lora_r"],
                lora_alpha=CONFIG["lora_alpha"],
                target_modules=["to_q", "to_k", "to_v", "to_out.0"],
                lora_dropout=CONFIG["lora_dropout"],
                bias="none",
            )

            # Apply LoRA to UNet
            unet = get_peft_model(unet, lora_config)
            print("Successfully applied LoRA with PEFT")

        except Exception as e:
            print(f"PEFT approach failed: {e}")
            print("Falling back to training without LoRA")
            # Reset processors to defaults
            unet.set_attn_processor({})

    return unet

# Train the weak model with LoRA
def train_on_batch(images, prompts, iteration):
    """Train the weak model on a batch of images with LoRA adaptation"""
    print(f"\nðŸ§  Training weak model (iteration {iteration+1})...")
    
    # Print current GPU memory usage
    print(f"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    
    # Aggressive memory cleanup
    torch.cuda.empty_cache()
    gc.collect()
    
    # Get components from pipeline
    unet = sd_pipe.unet
    text_encoder = sd_pipe.text_encoder
    tokenizer = sd_pipe.tokenizer
    scheduler = sd_pipe.scheduler
    
    # Print the expected input channels
    print(f"UNet expects {unet.conv_in.in_channels} input channels")
    
    # Process images with tiny dimensions
    smaller_size = 32  # Extremely small to save memory
    transform = transforms.Compose([
        transforms.Resize((smaller_size, smaller_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])
    
    # Process just a single image to minimize memory usage
    selected_img = images[0]
    selected_prompt = prompts[0]
    
    # Process just this one image
    img_tensor = transform(selected_img)
    img_tensor = torch.cat([img_tensor, img_tensor[0:1]], dim=0).unsqueeze(0)
    
    # Train only a tiny handful of parameters
    trainable_params = []
    
    # Use a deterministic approach instead of random
    for name, param in unet.named_parameters():
        # Only target a single specific attention layer for training
        if 'down_blocks.0.attentions.0.to_q' in name:
            param.requires_grad = True
            trainable_params.append(param)
            # Only train this one layer
            break
    
    # If no parameters were selected, force one
    if len(trainable_params) == 0:
        for name, param in unet.named_parameters():
            if '.to_q.' in name:  # Choose first query parameter
                param.requires_grad = True
                trainable_params.append(param)
                break
    
    print(f"Training {len(trainable_params)} parameters ({trainable_params[0].numel()} weights)")
    
    # Use memory-efficient optimizer
    optimizer = torch.optim.SGD(trainable_params, lr=1e-4)
    
    # Set environment variables for memory management
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    
    # Ultra simplified training - just 2 steps
    unet.train()
    loss_values = []
    
    try:
        # Process text once outside the loop to save memory
        with torch.no_grad():
            text_inputs = tokenizer(
                [selected_prompt],
                padding="max_length",
                max_length=tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt"
            ).input_ids.to(device)
            
            text_embeddings = text_encoder(text_inputs)[0]
            
        # Just 1 training step for simplicity
        # Check memory at each step
        print(f"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
        
        # Move tensors to GPU with the correct dtype
        # This is crucial: force everything to float16 to match the model's dtype
        img_tensor_gpu = img_tensor.to(device, dtype=torch.float16)
        
        # Add noise at a fixed timestep to simplify
        timestep = torch.tensor([500], device=device).long()
        # Use float16 for noise to match model dtype
        noise = torch.randn_like(img_tensor_gpu, dtype=torch.float16)
        noisy_image = scheduler.add_noise(img_tensor_gpu, noise, timestep)
        
        # Free unneeded memory
        del img_tensor_gpu
        torch.cuda.empty_cache()
        
        # Simple forward pass without autocast
        optimizer.zero_grad()
        
        # Ultra memory-efficient forward pass
        try:
            # Force all computations to float16
            with torch.autocast(device_type='cuda', dtype=torch.float16):
                noise_pred = unet(noisy_image, timestep, text_embeddings).sample
                # Make sure noise and noise_pred have same dtype
                loss = torch.nn.functional.mse_loss(noise_pred, noise, reduction='mean')
                
                # Backward pass
                loss.backward()
                
                # Update weights
                optimizer.step()
                
                # Record loss
                loss_val = loss.item()
                loss_values.append(loss_val)
                print(f"Step 1/1, Loss: {loss_val:.6f}")
        
        except RuntimeError as e:
            print(f"WARNING: Detailed error: {e}")
            traceback = import_module('traceback')
            traceback.print_exc()
            print("Skipping training due to error")
        
        # Immediate cleanup
        if 'noise_pred' in locals():
            del noise_pred
        if 'loss' in locals():
            del loss 
        del noisy_image, noise
        torch.cuda.empty_cache()
        gc.collect()
        
    except Exception as e:
        print(f"WARNING: Exception during training: {e}")
        if len(loss_values) == 0:
            loss_values = [0.0]
        
    finally:
        # Always clean up and return results
        metrics = {
            "train_loss": loss_values,
            "val_loss": [sum(loss_values)/max(len(loss_values), 1)],
            "learning_rate": [1e-4]
        }
        
        # Reset model to eval mode
        unet.eval()
        for param in unet.parameters():
            param.requires_grad = False
            
        del text_embeddings, text_inputs
        
        print("Training completed")
        print(f"GPU memory after training: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    
    return metrics, unet

# Calculate image similarity metrics
def calculate_image_similarity(img1, img2):
    """Calculate similarity metrics between two images"""
    # Convert PIL images to numpy arrays
    img1_np = np.array(img1)
    img2_np = np.array(img2)

    # Convert to grayscale for SSIM
    img1_gray = color.rgb2gray(img1_np)
    img2_gray = color.rgb2gray(img2_np)

    # 1. Mean Squared Error (lower is better)
    mse = np.mean((img1_np - img2_np) ** 2)

    # 2. Structural Similarity (higher is better)
    from skimage.metrics import structural_similarity as ssim
    ssim_score = ssim(img1_gray, img2_gray, data_range=1.0)

    # 3. Histogram similarity
    hist1 = cv2.calcHist([img1_np], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
    hist2 = cv2.calcHist([img2_np], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])

    cv2.normalize(hist1, hist1, 0, 1, cv2.NORM_MINMAX)
    cv2.normalize(hist2, hist2, 0, 1, cv2.NORM_MINMAX)

    hist_similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)

    # 4. Sharpness comparison
    from skimage import filters
    img1_laplacian_var = np.var(filters.laplace(img1_gray))
    img2_laplacian_var = np.var(filters.laplace(img2_gray))
    sharpness_ratio = img1_laplacian_var / (img2_laplacian_var + 1e-10)  # Avoid division by zero

    # Return all metrics
    return {
        "mse": mse,
        "ssim": ssim_score,
        "hist_similarity": hist_similarity,
        "sharpness_ratio": sharpness_ratio
    }

# Evaluate model with reduced memory usage
def evaluate_model(iteration):
    """Evaluate the model using standard prompts with memory optimization"""
    print(f"\nðŸ“Š Evaluating model (iteration {iteration+1})...")

    sd_pipe.unet.eval()  # Set to evaluation mode
    metrics = {}
    eval_prompts = CONFIG["eval_prompts"]

    # Generate images with both models
    weak_model_images = []
    strong_model_images = []

    for prompt in tqdm(eval_prompts, desc="Generating comparison images"):
        # Clear between each generation
        torch.cuda.empty_cache()
        gc.collect()
        
        # Generate with weak model
        with torch.inference_mode():
            weak_output = sd_pipe(
                prompt,
                num_inference_steps=CONFIG["inference_steps"],
                guidance_scale=CONFIG["guidance_scale"],
                height=CONFIG["image_size"],
                width=CONFIG["image_size"]
            )
            weak_img = weak_output.images[0]
            weak_model_images.append(weak_img)

        # Clear memory
        del weak_output
        torch.cuda.empty_cache()
        gc.collect()

        # Generate with strong model
        with torch.inference_mode():
            strong_output = sdxl_pipe(
                prompt,
                num_inference_steps=CONFIG["inference_steps"],
                guidance_scale=CONFIG["guidance_scale"],
                height=CONFIG["image_size"],
                width=CONFIG["image_size"]
            )
            strong_img = strong_output.images[0]
            strong_model_images.append(strong_img)

        # Clear memory
        del strong_output
        torch.cuda.empty_cache()
        gc.collect()

    # Calculate similarity metrics
    all_similarities = []

    for i, (weak_img, strong_img) in enumerate(zip(weak_model_images, strong_model_images)):
        # Calculate similarity metrics
        similarity = calculate_image_similarity(weak_img, strong_img)
        all_similarities.append(similarity)

        # Save comparison image
        comparison = Image.new('RGB', (CONFIG["image_size"]*2, CONFIG["image_size"]), color='white')
        comparison.paste(weak_img, (0, 0))
        comparison.paste(strong_img, (CONFIG["image_size"], 0))

        timestamp = int(time.time())
        save_path = BASE_DIR / "comparison_output" / f"eval_iter{iteration+1}_sample{i}_{timestamp}.png"
        comparison.save(save_path)

        # Save individual images
        weak_path = BASE_DIR / "weakmodel_images" / f"weak_iter{iteration+1}_sample{i}_{timestamp}.png"
        strong_path = BASE_DIR / "sdxl_images" / f"strong_eval_sample{i}_{timestamp}.png"
        weak_img.save(weak_path)
        strong_img.save(strong_path)

        # Save prompt
        with open(BASE_DIR / "comparison_output" / f"eval_iter{iteration+1}_sample{i}_{timestamp}.txt", "w") as f:
            f.write(eval_prompts[i])

    # Calculate average metrics
    avg_metrics = {}
    for key in all_similarities[0].keys():
        avg_metrics[key] = sum(s[key] for s in all_similarities) / len(all_similarities)

    # Log metrics
    print("\nEvaluation metrics:")
    for key, value in avg_metrics.items():
        print(f"{key}: {value:.4f}")

    # Save metrics
    metrics_path = BASE_DIR / "metrics" / f"metrics_iter_{iteration+1}.json"
    with open(metrics_path, "w") as f:
        json.dump(avg_metrics, f)

    # Create visualization
    fig, axes = plt.subplots(len(eval_prompts), 2, figsize=(12, 4*len(eval_prompts)))

    for i, (weak_img, strong_img) in enumerate(zip(weak_model_images, strong_model_images)):
        if len(eval_prompts) == 1:
            ax_weak = axes[0]
            ax_strong = axes[1]
        else:
            ax_weak = axes[i, 0]
            ax_strong = axes[i, 1]

        ax_weak.imshow(weak_img)
        ax_weak.set_title(f"Weak Model (iter {iteration+1})", fontsize=10)
        ax_weak.axis("off")

        ax_strong.imshow(strong_img)
        ax_strong.set_title(f"Strong Model (SDXL)", fontsize=10)
        ax_strong.axis("off")

        # Add metrics as text
        similarity = all_similarities[i]
        metrics_text = (
            f"MSE: {similarity['mse']:.1f}\n"
            f"SSIM: {similarity['ssim']:.3f}\n"
            f"Hist: {similarity['hist_similarity']:.3f}"
        )

        ax_weak.text(
            10, 20,
            metrics_text,
            color='white',
            bbox=dict(facecolor='black', alpha=0.7),
            fontsize=8
        )

        # Add prompt
        fig.text(0.5, 0.97 - (i * 0.97/len(eval_prompts)),
                f"Prompt: {eval_prompts[i][:50]}...",
                ha='center', fontsize=9,
                bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.5'))

    plt.tight_layout()
    plt.subplots_adjust(top=0.95)
    fig_path = BASE_DIR / "visualization" / f"comparison_iter_{iteration+1}.png"
    plt.savefig(fig_path, dpi=200)
    plt.close()

    # Create radar chart
    metrics_to_vis = {
        "SSIM": avg_metrics["ssim"],
        "Hist. Sim": avg_metrics["hist_similarity"],
        "Sharpness": min(avg_metrics["sharpness_ratio"], 2) / 2,  # Normalize to 0-1
        "MSE (inv)": 1 - min(avg_metrics["mse"] / 10000, 1)  # Invert and normalize
    }

    # Create radar chart
    fig = plt.figure(figsize=(6, 6))
    ax = fig.add_subplot(111, polar=True)

    # Set the angles for each metric
    angles = np.linspace(0, 2*np.pi, len(metrics_to_vis), endpoint=False).tolist()
    angles += angles[:1]  # Close the loop

    # Set the values
    values = list(metrics_to_vis.values())
    values += values[:1]  # Close the loop

    # Plot the values
    ax.plot(angles, values, 'o-', linewidth=2)
    ax.fill(angles, values, alpha=0.25)

    # Set the labels
    ax.set_thetagrids(np.degrees(angles[:-1]), list(metrics_to_vis.keys()))
    ax.set_ylim(0, 1)

    plt.title(f"Image Quality Metrics (Iteration {iteration+1})", size=12)
    radar_path = BASE_DIR / "visualization" / f"radar_metrics_iter_{iteration+1}.png"
    plt.savefig(radar_path, dpi=200)
    plt.close()

    return avg_metrics

# Plot training progress
def plot_training_progress(all_metrics):
    """Plot training progress metrics"""
    if not all_metrics:
        return

    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    iterations = list(range(1, len(all_metrics) + 1))

    # Plot MSE
    mse_values = [m["mse"] for m in all_metrics]
    axes[0, 0].plot(iterations, mse_values, 'o-', color='crimson')
    axes[0, 0].set_title('Mean Squared Error (lower is better)')
    axes[0, 0].set_xlabel('Iteration')
    axes[0, 0].set_ylabel('MSE')
    axes[0, 0].grid(True, alpha=0.3)

    # Plot SSIM
    ssim_values = [m["ssim"] for m in all_metrics]
    axes[0, 1].plot(iterations, ssim_values, 'o-', color='forestgreen')
    axes[0, 1].set_title('Structural Similarity (higher is better)')
    axes[0, 1].set_xlabel('Iteration')
    axes[0, 1].set_ylabel('SSIM')
    axes[0, 1].grid(True, alpha=0.3)

    # Plot histogram similarity
    hist_values = [m["hist_similarity"] for m in all_metrics]
    axes[1, 0].plot(iterations, hist_values, 'o-', color='darkorange')
    axes[1, 0].set_title('Histogram Similarity (higher is better)')
    axes[1, 0].set_xlabel('Iteration')
    axes[1, 0].set_ylabel('Similarity')
    axes[1, 0].grid(True, alpha=0.3)

    # Plot sharpness ratio
    sharpness_values = [m["sharpness_ratio"] for m in all_metrics]
    axes[1, 1].plot(iterations, sharpness_values, 'o-', color='royalblue')
    axes[1, 1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.7)
    axes[1, 1].set_title('Sharpness Ratio (closer to 1.0 is better)')
    axes[1, 1].set_xlabel('Iteration')
    axes[1, 1].set_ylabel('Ratio')
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    progress_path = BASE_DIR / "visualization" / "training_progress.png"
    plt.savefig(progress_path, dpi=200)
    plt.close()

    # Create summary table
    first_metrics = all_metrics[0]
    last_metrics = all_metrics[-1]

    improvement = {
        "mse": first_metrics["mse"] - last_metrics["mse"],
        "ssim": last_metrics["ssim"] - first_metrics["ssim"],
        "hist_similarity": last_metrics["hist_similarity"] - first_metrics["hist_similarity"],
        "sharpness_ratio": abs(1 - first_metrics["sharpness_ratio"]) - abs(1 - last_metrics["sharpness_ratio"])
    }

    # Calculate percentage improvement
    pct_improvement = {
        "mse": (improvement["mse"] / first_metrics["mse"]) * 100 if first_metrics["mse"] != 0 else 0,
        "ssim": (improvement["ssim"] / first_metrics["ssim"]) * 100 if first_metrics["ssim"] != 0 else 0,
        "hist_similarity": (improvement["hist_similarity"] / first_metrics["hist_similarity"]) * 100 if first_metrics["hist_similarity"] != 0 else 0,
        "sharpness_ratio": improvement["sharpness_ratio"] * 100  # Already a percentage improvement towards 1.0
    }

    fig, ax = plt.subplots(figsize=(10, 3))
    ax.axis('tight')
    ax.axis('off')

    cell_text = [
        ["MSE (â†“)", f"{first_metrics['mse']:.2f}", f"{last_metrics['mse']:.2f}", f"{improvement['mse']:.2f}", f"{pct_improvement['mse']:.1f}%"],
        ["SSIM (â†‘)", f"{first_metrics['ssim']:.4f}", f"{last_metrics['ssim']:.4f}", f"{improvement['ssim']:.4f}", f"{pct_improvement['ssim']:.1f}%"],
        ["Hist. Sim (â†‘)", f"{first_metrics['hist_similarity']:.4f}", f"{last_metrics['hist_similarity']:.4f}", f"{improvement['hist_similarity']:.4f}", f"{pct_improvement['hist_similarity']:.1f}%"],
        ["Sharpness (â†’1.0)", f"{first_metrics['sharpness_ratio']:.4f}", f"{last_metrics['sharpness_ratio']:.4f}", f"{improvement['sharpness_ratio']:.4f}", f"{pct_improvement['sharpness_ratio']:.1f}%"]
    ]

    table = ax.table(
        cellText=cell_text,
        colLabels=["Metric", "Initial", "Final", "Improvement", "% Change"],
        loc='center',
        cellLoc='center'
    )

    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)

    # Color code improvements
    for i, (key, imp) in enumerate(improvement.items()):
        # Different coloring based on metric (some better higher, some better lower)
        if key == "mse":
            # MSE: lower is better
            if imp > 0:  # Improved
                table[(i+1, 3)].set_facecolor('#d8f3dc')
                table[(i+1, 4)].set_facecolor('#d8f3dc')
            else:  # Worsened
                table[(i+1, 3)].set_facecolor('#ffccd5')
                table[(i+1, 4)].set_facecolor('#ffccd5')
        elif key == "sharpness_ratio":
            # Sharpness: closer to 1.0 is better
            if abs(last_metrics[key] - 1.0) < abs(first_metrics[key] - 1.0):  # Improved
                table[(i+1, 3)].set_facecolor('#d8f3dc')
                table[(i+1, 4)].set_facecolor('#d8f3dc')
            else:  # Worsened
                table[(i+1, 3)].set_facecolor('#ffccd5')
                table[(i+1, 4)].set_facecolor('#ffccd5')
        else:
            # SSIM, Histogram: higher is better
            if imp > 0:  # Improved
                table[(i+1, 3)].set_facecolor('#d8f3dc')
                table[(i+1, 4)].set_facecolor('#d8f3dc')
            else:  # Worsened
                table[(i+1, 3)].set_facecolor('#ffccd5')
                table[(i+1, 4)].set_facecolor('#ffccd5')

    plt.title("Training Improvement Summary", fontsize=14)
    summary_path = BASE_DIR / "visualization" / "improvement_summary.png"
    plt.savefig(summary_path, bbox_inches="tight", dpi=200)
    plt.close()

    # Calculate overall improvement score
    overall_improvement = (
        (pct_improvement["mse"] * 0.25) +          # 25% weight to MSE reduction
        (pct_improvement["ssim"] * 0.25) +         # 25% weight to SSIM improvement
        (pct_improvement["hist_similarity"] * 0.25) + # 25% weight to histogram improvement
        (pct_improvement["sharpness_ratio"] * 0.25)   # 25% weight to sharpness improvement
    )

    print(f"\nðŸ“ˆ Overall improvement score: {overall_improvement:.2f}%")
    return overall_improvement

# Generate final comparison report
def generate_final_report(all_metrics, start_time):
    """Generate a final report with before/after comparisons"""
    if not all_metrics or len(all_metrics) < 2:
        print("âš ï¸ Not enough data to generate a report")
        return

    # Create a report document
    report_path = BASE_DIR / "Final_Report.md"

    with open(report_path, "w") as f:
        # Header
        f.write("# Research Report: Improving Image Generation Quality via Knowledge Transfer\n\n")
        f.write(f"*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n")

        # Summary
        total_time = time.time() - start_time
        hours, remainder = divmod(total_time, 3600)
        minutes, seconds = divmod(remainder, 60)

        f.write("## Summary\n\n")
        f.write(f"- **Total training time**: {int(hours)}h {int(minutes)}m {int(seconds)}s\n")
        f.write(f"- **Total iterations**: {len(all_metrics)}\n")
        f.write(f"- **Images generated for training**: {len(all_metrics) * 25}\n")
        f.write(f"- **GPU used**: {torch.cuda.get_device_name(0)}\n\n")

        # Calculate improvement
        first_metrics = all_metrics[0]
        last_metrics = all_metrics[-1]

        improvement = {
            "mse": first_metrics["mse"] - last_metrics["mse"],
            "ssim": last_metrics["ssim"] - first_metrics["ssim"],
            "hist_similarity": last_metrics["hist_similarity"] - first_metrics["hist_similarity"],
            "sharpness_ratio": abs(1 - first_metrics["sharpness_ratio"]) - abs(1 - last_metrics["sharpness_ratio"])
        }

        # Result summary
        f.write("## Results\n\n")
        f.write("### Metrics Improvement\n\n")
        f.write("| Metric | Initial | Final | Improvement | Better When |\n")
        f.write("|--------|---------|-------|-------------|------------|\n")
        f.write(f"| Mean Squared Error | {first_metrics['mse']:.2f} | {last_metrics['mse']:.2f} | {improvement['mse']:.2f} | Lower |\n")
        f.write(f"| Structural Similarity | {first_metrics['ssim']:.4f} | {last_metrics['ssim']:.4f} | {improvement['ssim']:.4f} | Higher |\n")
        f.write(f"| Histogram Similarity | {first_metrics['hist_similarity']:.4f} | {last_metrics['hist_similarity']:.4f} | {improvement['hist_similarity']:.4f} | Higher |\n")
        f.write(f"| Sharpness Ratio | {first_metrics['sharpness_ratio']:.4f} | {last_metrics['sharpness_ratio']:.4f} | {improvement['sharpness_ratio']:.4f} | Closer to 1.0 |\n\n")

        # Include visualizations
        f.write("### Visualizations\n\n")
        f.write("#### Training Progress\n\n")
        f.write("![Training Progress](./research_dog_data/visualization/training_progress.png)\n\n")
        f.write("#### Final Comparison\n\n")
        f.write("![Final Comparison](./research_dog_data/visualization/comparison_iter_" + str(len(all_metrics)) + ".png)\n\n")
        f.write("#### Metrics Radar Chart\n\n")
        f.write("![Radar Chart](./research_dog_data/visualization/radar_metrics_iter_" + str(len(all_metrics)) + ".png)\n\n")

        # Conclusion
        f.write("## Conclusion\n\n")
        avg_improvement = sum(improvement.values()) / len(improvement)
        if avg_improvement > 0:
            f.write("The experiment successfully demonstrated that **knowledge transfer from a strong image generation model to a weaker model** is effective. By training on synthetic images generated by SDXL, the SD 1.5 model showed significant improvements in image quality metrics, particularly in structural similarity and color distribution.\n\n")
        else:
            f.write("The experiment showed mixed results. While some metrics improved, others did not show significant enhancement. Further optimization of the training parameters or longer training might be necessary to achieve more consistent improvements.\n\n")

        f.write("### Key Findings\n\n")
        if improvement["ssim"] > 0:
            f.write("- **Structural Similarity** showed the most consistent improvement, suggesting that the weak model learned to better replicate the structural elements of images produced by the strong model.\n")
        if improvement["hist_similarity"] > 0:
            f.write("- **Color distribution** became more similar to the strong model outputs, leading to more balanced and aesthetically pleasing images.\n")
        if abs(last_metrics["sharpness_ratio"] - 1.0) < abs(first_metrics["sharpness_ratio"] - 1.0):
            f.write("- **Image sharpness** moved closer to the reference level of the strong model, reducing issues with either over-sharpened or blurry outputs.\n")

        f.write("\n## Next Steps\n\n")
        f.write("1. **Extend training** to more iterations to observe if further improvements can be achieved\n")
        f.write("2. **Test with different subjects** beyond dogs to evaluate generalization capability\n")
        f.write("3. **Try different LoRA parameters** to optimize the adaptation process\n")
        f.write("4. **Perform human evaluation** to compare the subjective quality of generated images\n")

    print(f"âœ… Final report generated at {report_path}")
    return report_path

# First generate and save high-quality example images from SDXL
def generate_reference_images():
    """Generate high-quality reference images from SDXL"""
    print("\nðŸ–¼ï¸ Generating reference dog images with SDXL...")
    reference_images = []
    
    # Use more detailed prompts for better quality
    detailed_prompts = [
        "A golden retriever dog sitting in a park on a sunny day, highly detailed fur, realistic eyes, professional photography, shallow depth of field, high resolution, 8k",
        "A husky dog with blue eyes running through pristine snow at sunset, beautiful lighting, detailed fur texture, professional wildlife photography, high resolution, 8k",
        "A German shepherd dog with alert ears standing in a forest trail, dappled sunlight, detailed coat texture, professional photography, 8k, highly detailed",
        "A Labrador retriever playing with a ball at the beach, splashing water, golden hour lighting, detailed fur, action shot, professional photography, 8k"
    ]
    
    # Clear GPU cache
    torch.cuda.empty_cache()
    gc.collect()
    
    # Create a directory for reference images
    (BASE_DIR / "reference_images").mkdir(exist_ok=True)
    
    for i, prompt in enumerate(detailed_prompts):
        print(f"Generating reference image {i+1}/{len(detailed_prompts)}")
        
        # Set a specific seed for each image
        generator = torch.Generator(device=device).manual_seed(CONFIG["seed"] + i)
        
        try:
            with torch.inference_mode():
                # Use high-quality settings
                output = sdxl_pipe(
                    prompt,
                    num_inference_steps=CONFIG["inference_steps"],
                    guidance_scale=CONFIG["guidance_scale"],
                    height=CONFIG["image_size"],
                    width=CONFIG["image_size"],
                    generator=generator
                )
                
                image = output.images[0]
                reference_images.append(image)
                
                # Save the image with its prompt
                image_path = BASE_DIR / "reference_images" / f"reference_dog_{i+1}.png"
                image.save(image_path)
                
                with open(BASE_DIR / "reference_images" / f"reference_dog_{i+1}.txt", "w") as f:
                    f.write(prompt)
                
                print(f"âœ“ Saved reference image {i+1}")
            
            # Clean up GPU memory
            del output
            torch.cuda.empty_cache()
            gc.collect()
            
        except Exception as e:
            print(f"Error generating reference image: {e}")
            import traceback
            traceback.print_exc()
    
    return reference_images, detailed_prompts

# Function to train model with proper LoRA setup
def setup_and_train_lora(reference_images, prompts):
    """Set up and train LoRA on the weak model using reference images"""
    print("\nðŸ”„ Setting up LoRA for fine-tuning...")
    
    # Save original unet state dict for comparison later
    orig_unet_state_dict = copy.deepcopy(sd_pipe.unet.state_dict())
    
    # Import necessary libraries
    try:
        import peft
        print("PEFT library is already installed")
    except ImportError:
        print("Installing PEFT library for LoRA support...")
        !pip install -q peft
        import peft
        
    from peft import LoraConfig
    from diffusers.utils import make_image_grid

    # Set up LoRA configuration
    lora_config = LoraConfig(
        r=CONFIG["lora_r"],
        lora_alpha=CONFIG["lora_alpha"],
        target_modules=["to_q", "to_k", "to_v"],
        lora_dropout=0.1,
        bias="none"
    )
    
    # Create a clean UNet from the pipeline
    unet = sd_pipe.unet
    
    # Create PEFT model
    from peft import get_peft_model
    unet_lora = get_peft_model(unet, lora_config)
    
    # Training parameters
    trainable_params = [p for p in unet_lora.parameters() if p.requires_grad]
    print(f"Training {len(trainable_params)} parameters")
    
    # Create optimizer
    optimizer = torch.optim.AdamW(trainable_params, lr=5e-5)
    
    # Start training
    unet_lora.train()
    print("Starting training with PEFT LoRA...")
    
    # Define number of epochs
    num_train_epochs = 50
    
    for epoch in range(num_train_epochs):
        epoch_loss = 0
        
        # Process each image
        for img_idx, (image, prompt) in enumerate(zip(reference_images, prompts)):
            # Convert image to tensor
            image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(device)
            if image_tensor.shape[1] == 4:  # RGBA
                image_tensor = image_tensor[:, :3, :, :]  # Remove alpha
            image_tensor = image_tensor.to(dtype=torch.float16)
            
            # Create latents
            with torch.no_grad():
                latents = sd_pipe.vae.encode(image_tensor).latent_dist.sample() * 0.18215
            
            # Create text embeddings
            text_input = sd_pipe.tokenizer(
                [prompt],
                padding="max_length",
                max_length=sd_pipe.tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt"
            ).input_ids.to(device)
            
            with torch.no_grad():
                text_embeddings = sd_pipe.text_encoder(text_input)[0]
            
            # Add noise
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, sd_pipe.scheduler.config.num_train_timesteps, (1,), device=device)
            noisy_latents = sd_pipe.scheduler.add_noise(latents, noise, timesteps)
            
            # Train step
            optimizer.zero_grad()
            
            # Get prediction and calculate loss
            noise_pred = unet_lora(noisy_latents, timesteps, text_embeddings).sample
            loss = F.mse_loss(noise_pred, noise, reduction="mean")
            
            # Backpropagate
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
            # Clean up
            del latents, text_embeddings, noisy_latents, noise_pred, loss
            torch.cuda.empty_cache()
            gc.collect()
        
        # Log progress
        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch+1}/{num_train_epochs}, Loss: {epoch_loss:.6f}")
    
    # Set model back to eval mode
    unet_lora.eval()
    
    # Save LoRA weights only
    lora_path = BASE_DIR / "checkpoints"
    lora_path.mkdir(exist_ok=True, parents=True)
    
    # Save the LoRA weights (not the full model)
    unet_lora.save_pretrained(lora_path / "lora_weights")
    
    # Update the pipeline with the LoRA model
    sd_pipe.unet = unet_lora
    
    return orig_unet_state_dict

def generate_comparison_images(orig_unet_state_dict):
    """Generate comparison images to show improvement"""
    print("\nðŸ“Š Generating comparison images...")
    
    # Create directory for comparison images
    comparison_dir = BASE_DIR / "comparison_results"
    comparison_dir.mkdir(exist_ok=True, parents=True)
    
    # Test prompts
    test_prompts = [
        "A golden retriever dog sitting in a park on a sunny day, highly detailed photograph",
        "A husky dog running in snow at sunset, professional photography",
        "A black labrador retriever swimming in a lake, action shot, detailed fur"
    ]
    
    # Make a copy of the current (fine-tuned) model state
    finetuned_pipeline = copy.deepcopy(sd_pipe)
    
    # Create a new pipeline for original model
    from diffusers import StableDiffusionPipeline
    
    # Load a fresh pipeline for comparison to avoid model state conflicts
    original_pipeline = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16,
        safety_checker=None
    ).to(device)
    
    # Optional: Set up memory efficient attention
    if hasattr(original_pipeline, "enable_attention_slicing"):
        original_pipeline.enable_attention_slicing()
    
    print("Generating images with original model...")
    for i, prompt in enumerate(test_prompts):
        try:
            # Set seed for reproducibility
            generator = torch.Generator(device=device).manual_seed(CONFIG["seed"] + i)
            
            # Generate image
            with torch.inference_mode():
                image = original_pipeline(
                    prompt,
                    num_inference_steps=30,
                    guidance_scale=7.5,
                    generator=generator
                ).images[0]
                
                # Save original image
                image_path = comparison_dir / f"original_{i+1}.png"
                image.save(image_path)
                print(f"âœ“ Saved original model image {i+1}")
            
            # Free memory
            torch.cuda.empty_cache()
            gc.collect()
            
        except Exception as e:
            print(f"Error generating original image: {e}")
    
    # Generate with fine-tuned model
    print("Generating images with fine-tuned model...")
    for i, prompt in enumerate(test_prompts):
        try:
            # Set same seed for fair comparison
            generator = torch.Generator(device=device).manual_seed(CONFIG["seed"] + i)
            
            # Generate image
            with torch.inference_mode():
                image = finetuned_pipeline(
                    prompt,
                    num_inference_steps=30,
                    guidance_scale=7.5,
                    generator=generator
                ).images[0]
                
                # Save fine-tuned image
                image_path = comparison_dir / f"finetuned_{i+1}.png"
                image.save(image_path)
                print(f"âœ“ Saved fine-tuned model image {i+1}")
            
            # Free memory
            torch.cuda.empty_cache()
            gc.collect()
            
        except Exception as e:
            print(f"Error generating fine-tuned image: {e}")
    
    print(f"âœ“ All comparison images saved to {comparison_dir}")
    return None, None

# Main function
def main():
    """Main function to generate high-quality dog images and fine-tune model"""
    print("\nðŸš€ Starting high-quality image generation and fine-tuning experiment")
    
    try:
        # Generate reference images from SDXL
        reference_images, detailed_prompts = generate_reference_images()
        
        if len(reference_images) == 0:
            print("âŒ Failed to generate any reference images. Exiting.")
            return
        
        # Fine-tune the weak model with LoRA
        orig_unet_state_dict = setup_and_train_lora(reference_images, detailed_prompts)
        
        # Generate comparison images
        generate_comparison_images(orig_unet_state_dict)
        
        print("\nâœ… Process completed successfully!")
        print(f"ðŸ–¼ï¸ Check the generated images in: {BASE_DIR}/reference_images and {BASE_DIR}/comparison_results")
    
    except Exception as e:
        print(f"\nâŒ Error during execution: {e}")
        import traceback
        traceback.print_exc()
        
        print("\nðŸ”„ Generating standard dog images without training...")
        generate_standard_dog_images()

# Function to generate standard dog images without training
def generate_standard_dog_images():
    """Generate standard dog images with both models if training fails"""
    # Create output directory
    output_dir = BASE_DIR / "standard_outputs"
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Test prompts
    test_prompts = [
        "A golden retriever dog sitting in a park on a sunny day, highly detailed photograph",
        "A husky dog running in snow at sunset, professional photography"
    ]
    
    # Generate with SD 1.5
    for i, prompt in enumerate(test_prompts):
        try:
            # Set seed for reproducibility
            generator = torch.Generator(device=device).manual_seed(42 + i)
            
            # Generate image
            with torch.inference_mode():
                image = sd_pipe(
                    prompt,
                    num_inference_steps=30,
                    guidance_scale=7.5,
                    generator=generator
                ).images[0]
                
                # Save image
                image_path = output_dir / f"sd15_dog_{i+1}.png"
                image.save(image_path)
                print(f"âœ“ Saved SD 1.5 image {i+1}")
            
            # Free memory
            torch.cuda.empty_cache()
            gc.collect()
            
        except Exception as e:
            print(f"Error generating SD 1.5 image: {e}")
    
    # Generate with SDXL if available
    if 'sdxl_pipe' in globals():
        for i, prompt in enumerate(test_prompts):
            try:
                # Set seed for reproducibility
                generator = torch.Generator(device=device).manual_seed(42 + i)
                
                # Generate image
                with torch.inference_mode():
                    image = sdxl_pipe(
                        prompt,
                        num_inference_steps=30,
                        guidance_scale=7.5,
                        generator=generator
                    ).images[0]
                    
                    # Save image
                    image_path = output_dir / f"sdxl_dog_{i+1}.png"
                    image.save(image_path)
                    print(f"âœ“ Saved SDXL image {i+1}")
                
                # Free memory
                torch.cuda.empty_cache()
                gc.collect()
                
            except Exception as e:
                print(f"Error generating SDXL image: {e}")
    
    print(f"âœ“ Standard images saved to {output_dir}")

if __name__ == "__main__":
    main()