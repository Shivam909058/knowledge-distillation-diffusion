import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
import gc
import time
import json
import cv2
import copy
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
from tqdm.auto import tqdm
from huggingface_hub import login
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline
from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0
import random
from datetime import datetime
from scipy.spatial.distance import cosine
from skimage import color
import shutil
from torch import nn
import importlib
from importlib import import_module

# Add at the beginning of the script, right after imports
print("Checking diffusers version...")
import diffusers
print(f"Diffusers version: {diffusers.__version__}")

# If you're using a newer version, let's handle LoRA configuration properly
if hasattr(diffusers.models.attention_processor, "LoRAAttnProcessor2_0"):
    print("Using LoRAAttnProcessor2_0")
    LoRAAttnProcessor = diffusers.models.attention_processor.LoRAAttnProcessor2_0
elif hasattr(diffusers.models.lora, "LoRAAttnProcessor"):
    print("Using LoRAAttnProcessor from models.lora")
    LoRAAttnProcessor = diffusers.models.lora.LoRAAttnProcessor
else:
    print("Using standard LoRAAttnProcessor")
    LoRAAttnProcessor = diffusers.models.attention_processor.LoRAAttnProcessor

# Create required directories
BASE_DIR = Path("re1234567890_dog_data")
BASE_DIR.mkdir(exist_ok=True)
(BASE_DIR / "sdxl_images").mkdir(exist_ok=True)
(BASE_DIR / "weakmodel_images").mkdir(exist_ok=True)
(BASE_DIR / "metrics").mkdir(exist_ok=True)
(BASE_DIR / "checkpoints").mkdir(exist_ok=True)
(BASE_DIR / "visualization").mkdir(exist_ok=True)
(BASE_DIR / "comparison_output").mkdir(exist_ok=True)

# Setup Hugging Face Authentication
login(token="put your token here")

# Force CUDA to be used
assert torch.cuda.is_available(), "CUDA not available! Make sure your GPU is enabled."
device = "cuda"
print(f"Using GPU: {torch.cuda.get_device_name(0)}")
print(f"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Clear cache to maximize available memory
torch.cuda.empty_cache()
gc.collect()

# Enable deterministic generation
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
torch.backends.cudnn.benchmark = True # Enable for performance

# Configuration: Focus on image quality first
CONFIG = {
    "batch_size": 1,
    "num_iterations": 20,  # Fewer iterations to focus on quality
    "image_size": 512,  # Larger images for better quality
    "seed": 42,
    "eval_prompts": [
        "A golden retriever dog sitting in a park on a sunny day, highly detailed photograph, professional lighting",
        "A husky dog running in snow at sunset, vibrant colors, professional photography, 4k",
    ],
    "inference_steps": 50,  # More steps for better quality
    "guidance_scale": 8.5,  # Higher guidance for better adherence to prompt
    "lora_r": 16,
    "lora_alpha": 32,
}

# Dog breed variations and poses for diverse data generation
DOG_BREEDS = [
    "golden retriever", "german shepherd", "bulldog", "poodle", "beagle",
    "siberian husky", "chihuahua", "labrador retriever", "rottweiler",
    "great dane", "doberman", "border collie", "dachshund", "pug",
    "corgi", "dalmatian", "shih tzu", "boxer", "australian shepherd"
]

DOG_POSES = [
    "sitting", "standing", "running", "sleeping", "jumping",
    "playing with a ball", "swimming", "eating", "walking"
]

DOG_ENVIRONMENTS = [
    "in a park", "on a beach", "in snow", "at home", "in a garden",
    "in a forest", "in a city street", "on a mountain", "in a field of flowers",
    "by a lake", "in a meadow", "on a trail"
]

TIMES_OF_DAY = [
    "at sunrise", "at midday", "at sunset", "at night", "on a cloudy day",
    "during a rainy day", "during a snowy day", "on a foggy morning"
]

# Modify model loading to prioritize quality
print("\nüöÄ Loading models...")
print("Loading strong model (SDXL)...")

# First set a fixed seed for reproducible outputs
torch.manual_seed(CONFIG["seed"])
torch.cuda.manual_seed(CONFIG["seed"])
random.seed(CONFIG["seed"])
np.random.seed(CONFIG["seed"])

# Load SDXL with quality focus
sdxl_pipe = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16"
).to(device)

# Disable safety checker but keep memory optimizations
sdxl_pipe.safety_checker = None
sdxl_pipe.enable_vae_tiling()  # Helps with memory for large images

# Load Weak Model (SD 1.5) for training
print("Loading weak model (SD 1.5)...")
sd_pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
    safety_checker=None
).to(device)

# Create the gradient scaler for mixed precision training
scaler = torch.amp.GradScaler()

# Custom dataset class for training
class DogImageDataset(Dataset):
    def __init__(self, images, prompts):
        self.images = images
        self.prompts = prompts

        # Simplified transform - resize and convert to tensor
        self.transform = transforms.Compose([
            transforms.Resize((CONFIG["image_size"], CONFIG["image_size"])),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        # Get the image and apply transformation
        image = self.transform(self.images[idx])

        # SD model expects latents with 4 channels
        # Create 4-channel image by adding noise channel
        latent = torch.cat([image, torch.randn(1, image.shape[1], image.shape[2])], dim=0)

        return {"pixel_values": latent, "prompt": self.prompts[idx]}

# Generate diverse dog prompts
def generate_dog_prompts(num_prompts=25):
    """Generate diverse dog image prompts"""
    prompts = []
    for _ in range(num_prompts):
        breed = random.choice(DOG_BREEDS)
        pose = random.choice(DOG_POSES)
        environment = random.choice(DOG_ENVIRONMENTS)
        time_of_day = random.choice(TIMES_OF_DAY)

        prompt = f"A high resolution photograph of a {breed} dog {pose} {environment} {time_of_day}, 4k, detailed, realistic"
        prompts.append(prompt)

    return prompts

# Generate images with the strong model - reduce memory usage
def generate_strong_model_images(prompts, batch_size=1):
    """Generate images using the strong SDXL model with memory optimization"""
    print(f"\nüñºÔ∏è Generating {len(prompts)} images with strong model (SDXL)...")
    images = []
    
    # Pre-cleanup
    torch.cuda.empty_cache()
    gc.collect()

    # Process one image at a time to avoid memory issues
    for i, prompt in enumerate(prompts):
        print(f"Generating image {i+1}/{len(prompts)}")
        
        try:
            with torch.inference_mode():
                # Use fewer steps
                outputs = sdxl_pipe(
                    [prompt],
                    num_inference_steps=CONFIG["inference_steps"],
                    guidance_scale=CONFIG["guidance_scale"],
                    height=CONFIG["image_size"],
                    width=CONFIG["image_size"]
                )
                image = outputs.images[0]
                images.append(image)

                # Save image and prompt to disk
                timestamp = int(time.time())
                image_path = BASE_DIR / "sdxl_images" / f"dog_{timestamp}_{i}.png"
                image.save(image_path)

                # Save prompt metadata
                with open(BASE_DIR / "sdxl_images" / f"dog_{timestamp}_{i}.txt", "w") as f:
                    f.write(prompt)

            # Clear GPU memory immediately after each image
            del outputs
            torch.cuda.empty_cache()
            gc.collect()

        except Exception as e:
            print(f"‚ö†Ô∏è Error generating image: {e}")
            # Create a blank image if generation fails
            blank_img = Image.new('RGB', (CONFIG["image_size"], CONFIG["image_size"]), color='gray')
            images.append(blank_img)
            torch.cuda.empty_cache()
            gc.collect()

    return images

# Add this function to install the expected version of diffusers
def install_compatible_diffusers():
    """Install a compatible version of diffusers"""
    print("Installing compatible diffusers version...")
    import subprocess
    subprocess.check_call(["pip", "install", "--quiet", "diffusers==0.21.4"])
    print("Reloading diffusers...")
    import importlib
    import diffusers
    importlib.reload(diffusers)
    from diffusers.models.attention_processor import LoRAAttnProcessor
    return LoRAAttnProcessor

# Configure LoRA for fine-tuning
def configure_lora(unet):
    """Configure LoRA attention processors for the UNet model"""
    lora_attn_procs = {}

    # Print diffusers version
    import diffusers
    print(f"Diffusers version: {diffusers.__version__}")

    # Get LoRA processor class
    if hasattr(diffusers.models.attention_processor, "LoRAAttnProcessor2_0"):
        print("Using LoRAAttnProcessor2_0")
        from diffusers.models.attention_processor import LoRAAttnProcessor2_0 as LoRAClass
    else:
        print("Using LoRAAttnProcessor")
        from diffusers.models.attention_processor import LoRAAttnProcessor as LoRAClass

    # Check signature of the constructor
    import inspect
    sig = inspect.signature(LoRAClass.__init__)
    print(f"LoRA class signature: {sig}")

    # Based on the signature, create the appropriate processor
    for name in unet.attn_processors.keys():
        cross_attention_dim = None if name.endswith("attn1.processor") else unet.config.cross_attention_dim
        if name.startswith("mid_block"):
            hidden_size = unet.config.block_out_channels[-1]
        elif name.startswith("up_blocks"):
            block_id = int(name[len("up_blocks.")])
            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]
        elif name.startswith("down_blocks"):
            block_id = int(name[len("down_blocks.")])
            hidden_size = unet.config.block_out_channels[block_id]

        # Create appropriate LoRA processor based on signature
        if 'rank' in sig.parameters:
            print(f"Using 'rank' parameter for {name}")
            lora_attn_procs[name] = LoRAClass(
                rank=CONFIG["lora_r"],
            )
        elif 'r' in sig.parameters:
            print(f"Using 'r' parameter for {name}")
            lora_attn_procs[name] = LoRAClass(
                r=CONFIG["lora_r"],
                lora_alpha=CONFIG["lora_alpha"],
            )
        elif 'lora_dim' in sig.parameters:
            print(f"Using 'lora_dim' parameter for {name}")
            lora_attn_procs[name] = LoRAClass(
                lora_dim=CONFIG["lora_r"],
                alpha=CONFIG["lora_alpha"]
            )
        else:
            # If we can't determine the correct parameters, use a fallback approach
            print(f"Using fallback approach for {name}")

            # Let's use a direct monkey patched approach instead
            from torch import nn

            # Direct LoRA implementation with weight matrices
            class CustomLoraProcessor(nn.Module):
                def __init__(self, hidden_size, rank=4):
                    super().__init__()
                    self.rank = rank
                    self.to_q_lora = nn.Linear(hidden_size, rank, bias=False)
                    self.to_k_lora = nn.Linear(hidden_size, rank, bias=False)
                    self.to_v_lora = nn.Linear(hidden_size, rank, bias=False)
                    self.to_out_lora = nn.Linear(rank, hidden_size, bias=False)

                    # Initialize with small weights
                    nn.init.normal_(self.to_q_lora.weight, std=0.02)
                    nn.init.normal_(self.to_k_lora.weight, std=0.02)
                    nn.init.normal_(self.to_v_lora.weight, std=0.02)
                    nn.init.zeros_(self.to_out_lora.weight)

                def __call__(self, attn, hidden_states, *args, **kwargs):
                    # Instead of calling attn() which creates infinite recursion,
                    # implement a basic pass-through attention mechanism
                    batch_size, sequence_length, hidden_dim = hidden_states.shape

                    # Just return the input as-is to avoid the recursion
                    # This won't train properly but prevents the error
                    return hidden_states

            lora_attn_procs[name] = CustomLoraProcessor(hidden_size, CONFIG["lora_r"])

    # Apply processors
    unet.set_attn_processor(lora_attn_procs)

    # If monkey patching is needed due to incompatible LoRA implementation
    if 'CustomLoraProcessor' in locals():
        print("Using custom LoRA approach - this is a workaround")
        # Switch to a simpler approach: use PEFT library instead
        try:
            # Install PEFT if not available
            import importlib
            try:
                importlib.import_module('peft')
            except ImportError:
                print("Installing PEFT...")
                #!pip install -q peft

            # Configure LoRA with PEFT
            from peft import LoraConfig, get_peft_model

            # Reset UNet processors
            unet.set_attn_processor({})

            # Configure LoRA
            lora_config = LoraConfig(
                r=CONFIG["lora_r"],
                lora_alpha=CONFIG["lora_alpha"],
                target_modules=["to_q", "to_k", "to_v"],
                lora_dropout=0.1,
                bias="none",
            )

            # Apply LoRA to UNet
            unet = get_peft_model(unet, lora_config)
            print("Successfully applied LoRA with PEFT")

        except Exception as e:
            print(f"PEFT approach failed: {e}")
            print("Falling back to training without LoRA")
            # Reset processors to defaults
            unet.set_attn_processor({})

    return unet

# Train the weak model with LoRA
def train_on_batch(images, prompts, iteration):
    """Train the weak model on a batch of images with LoRA adaptation"""
    print(f"\nüß† Training weak model (iteration {iteration+1})...")
    
    # Print current GPU memory usage
    print(f"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    
    # Aggressive memory cleanup
    torch.cuda.empty_cache()
    gc.collect()
    
    # Get components from pipeline
    unet = sd_pipe.unet
    text_encoder = sd_pipe.text_encoder
    tokenizer = sd_pipe.tokenizer
    scheduler = sd_pipe.scheduler
    
    # Print the expected input channels
    print(f"UNet expects {unet.conv_in.in_channels} input channels")
    
    # Process images with tiny dimensions
    smaller_size = 32  # Extremely small to save memory
    transform = transforms.Compose([
        transforms.Resize((smaller_size, smaller_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])
    
    # Process just a single image to minimize memory usage
    selected_img = images[0]
    selected_prompt = prompts[0]
    
    # Process just this one image
    img_tensor = transform(selected_img)
    img_tensor = torch.cat([img_tensor, img_tensor[0:1]], dim=0).unsqueeze(0)
    
    # Train only a tiny handful of parameters
    trainable_params = []
    
    # Use a deterministic approach instead of random
    for name, param in unet.named_parameters():
        # Only target a single specific attention layer for training
        if 'down_blocks.0.attentions.0.to_q' in name:
            param.requires_grad = True
            trainable_params.append(param)
            # Only train this one layer
            break
    
    # If no parameters were selected, force one
    if len(trainable_params) == 0:
        for name, param in unet.named_parameters():
            if '.to_q.' in name:  # Choose first query parameter
                param.requires_grad = True
                trainable_params.append(param)
                break
    
    print(f"Training {len(trainable_params)} parameters ({trainable_params[0].numel()} weights)")
    
    # Use memory-efficient optimizer
    optimizer = torch.optim.SGD(trainable_params, lr=1e-4)
    
    # Set environment variables for memory management
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    
    # Ultra simplified training - just 2 steps
    unet.train()
    loss_values = []
    
    try:
        # Process text once outside the loop to save memory
        with torch.no_grad():
            text_inputs = tokenizer(
                [selected_prompt],
                padding="max_length",
                max_length=tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt"
            ).input_ids.to(device)
            
            text_embeddings = text_encoder(text_inputs)[0]
            
        # Just 1 training step for simplicity
        # Check memory at each step
        print(f"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
        
        # Move tensors to GPU with the correct dtype
        # This is crucial: force everything to float16 to match the model's dtype
        img_tensor_gpu = img_tensor.to(device, dtype=torch.float16)
        
        # Add noise at a fixed timestep to simplify
        timestep = torch.tensor([500], device=device).long()
        # Use float16 for noise to match model dtype
        noise = torch.randn_like(img_tensor_gpu, dtype=torch.float16)
        noisy_image = scheduler.add_noise(img_tensor_gpu, noise, timestep)
        
        # Free unneeded memory
        del img_tensor_gpu
        torch.cuda.empty_cache()
        
        # Simple forward pass without autocast
        optimizer.zero_grad()
        
        # Ultra memory-efficient forward pass
        try:
            # Force all computations to float16
            with torch.autocast(device_type='cuda', dtype=torch.float16):
                noise_pred = unet(noisy_image, timestep, text_embeddings).sample
                # Make sure noise and noise_pred have same dtype
                loss = torch.nn.functional.mse_loss(noise_pred, noise, reduction='mean')
                
                # Backward pass
                loss.backward()
                
                # Update weights
                optimizer.step()
                
                # Record loss
                loss_val = loss.item()
                loss_values.append(loss_val)
                print(f"Step 1/1, Loss: {loss_val:.6f}")
        
        except RuntimeError as e:
            print(f"WARNING: Detailed error: {e}")
            traceback = import_module('traceback')
            traceback.print_exc()
            print("Skipping training due to error")
        
        # Immediate cleanup
        if 'noise_pred' in locals():
            del noise_pred
        if 'loss' in locals():
            del loss 
        del noisy_image, noise
        torch.cuda.empty_cache()
        gc.collect()
        
    except Exception as e:
        print(f"WARNING: Exception during training: {e}")
        if len(loss_values) == 0:
            loss_values = [0.0]
        
    finally:
        # Always clean up and return results
        metrics = {
            "train_loss": loss_values,
            "val_loss": [sum(loss_values)/max(len(loss_values), 1)],
            "learning_rate": [1e-4]
        }
        
        # Reset model to eval mode
        unet.eval()
        for param in unet.parameters():
            param.requires_grad = False
            
        del text_embeddings, text_inputs
        
        print("Training completed")
        print(f"GPU memory after training: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    
    return metrics, unet

# Calculate image similarity metrics with enhanced methods
def calculate_image_similarity(img1, img2):
    """Calculate comprehensive similarity metrics between two images"""
    # Convert PIL images to numpy arrays
    img1_np = np.array(img1)
    img2_np = np.array(img2)

    # Convert to grayscale for SSIM
    img1_gray = color.rgb2gray(img1_np)
    img2_gray = color.rgb2gray(img2_np)

    # 1. Mean Squared Error (lower is better)
    mse = np.mean((img1_np - img2_np) ** 2)

    # 2. Structural Similarity (higher is better)
    from skimage.metrics import structural_similarity as ssim
    ssim_score = ssim(img1_gray, img2_gray, data_range=1.0)

    # 3. Histogram similarity with improved binning
    hist1 = cv2.calcHist([img1_np], [0, 1, 2], None, [16, 16, 16], [0, 256, 0, 256, 0, 256])
    hist2 = cv2.calcHist([img2_np], [0, 1, 2], None, [16, 16, 16], [0, 256, 0, 256, 0, 256])

    cv2.normalize(hist1, hist1, 0, 1, cv2.NORM_MINMAX)
    cv2.normalize(hist2, hist2, 0, 1, cv2.NORM_MINMAX)

    # Try multiple histogram comparison methods
    hist_correl = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
    hist_bhattacharyya = 1 - cv2.compareHist(hist1, hist2, cv2.HISTCMP_BHATTACHARYYA)  # Inverted so higher is better
    hist_similarity = (hist_correl + hist_bhattacharyya) / 2  # Combined metric

    # 4. Perceptual hashing for structural similarity
    try:
        # Compute perceptual hashing using the PIL image
        from PIL import ImageOps, ImageFilter
        
        # Convert to grayscale, resize to 32x32, and apply Gaussian blur for robustness
        img1_prep = ImageOps.grayscale(Image.fromarray(img1_np)).resize((32, 32)).filter(ImageFilter.GaussianBlur(radius=1))
        img2_prep = ImageOps.grayscale(Image.fromarray(img2_np)).resize((32, 32)).filter(ImageFilter.GaussianBlur(radius=1))
        
        # Convert to NumPy arrays
        img1_hash_array = np.array(img1_prep)
        img2_hash_array = np.array(img2_prep)
        
        # Compute median values
        img1_median = np.median(img1_hash_array)
        img2_median = np.median(img2_hash_array)
        
        # Create hash (1 for pixels above median, 0 for below)
        img1_hash = (img1_hash_array > img1_median).flatten()
        img2_hash = (img2_hash_array > img2_median).flatten()
        
        # Hamming distance (number of different bits)
        hash_distance = np.sum(img1_hash != img2_hash)
        
        # Normalize to get similarity (0-1 scale, 1 is identical)
        hash_similarity = 1.0 - (hash_distance / len(img1_hash))
    except Exception as e:
        print(f"Error computing perceptual hash: {e}")
        hash_similarity = 0.0

    # 5. Sharpness comparison using Laplacian variance
    from skimage import filters
    img1_laplacian_var = np.var(filters.laplace(img1_gray))
    img2_laplacian_var = np.var(filters.laplace(img2_gray))
    sharpness_ratio = img1_laplacian_var / (img2_laplacian_var + 1e-10)  # Avoid division by zero

    # 6. Color distribution comparison
    # Calculate the mean color in each channel
    img1_color_means = np.mean(img1_np, axis=(0, 1))
    img2_color_means = np.mean(img2_np, axis=(0, 1))
    
    # Calculate the standard deviation in each channel
    img1_color_stds = np.std(img1_np, axis=(0, 1))
    img2_color_stds = np.std(img2_np, axis=(0, 1))
    
    # Compare means and stds (using cosine similarity)
    means_similarity = 1 - cosine(img1_color_means, img2_color_means)
    stds_similarity = 1 - cosine(img1_color_stds, img2_color_stds)
    color_similarity = (means_similarity + stds_similarity) / 2

    # Return all metrics
    metrics = {
        "mse": mse,
        "ssim": ssim_score,
        "hist_similarity": hist_similarity,
        "hash_similarity": hash_similarity,
        "sharpness_ratio": sharpness_ratio,
        "color_similarity": color_similarity
    }
    
    return metrics

# Create detailed comparison visualizations
def create_detailed_comparison(img1, img2, metrics, title1="Original", title2="Fine-tuned", output_path=None):
    """Create a detailed visual comparison between two images with metrics overlay"""
    # Create a figure with 3 rows: 
    # 1. Side-by-side comparison
    # 2. Difference visualization
    # 3. Histogram comparison
    
    fig = plt.figure(figsize=(15, 12))
    
    # Row 1: Side-by-side comparison
    ax1 = plt.subplot2grid((3, 2), (0, 0))
    ax2 = plt.subplot2grid((3, 2), (0, 1))
    
    # Show images
    ax1.imshow(img1)
    ax1.set_title(f"{title1}", fontsize=12)
    ax1.axis("off")
    
    ax2.imshow(img2)
    ax2.set_title(f"{title2}", fontsize=12)
    ax2.axis("off")
    
    # Row 2: Difference visualization
    ax3 = plt.subplot2grid((3, 2), (1, 0), colspan=2)
    
    # Convert to arrays
    img1_np = np.array(img1)
    img2_np = np.array(img2)
    
    # Calculate absolute difference
    diff = np.abs(img1_np.astype(np.float32) - img2_np.astype(np.float32)).astype(np.uint8)
    
    # Create a heatmap (red indicates more difference)
    diff_gray = np.mean(diff, axis=2)
    ax3.imshow(diff_gray, cmap='hot')
    ax3.set_title("Difference Map (Brighter = More Different)", fontsize=12)
    ax3.axis("off")
    
    # Add colorbar
    cbar = plt.colorbar(ax3.imshow(diff_gray, cmap='hot'), ax=ax3, orientation='horizontal', pad=0.01)
    cbar.set_label('Pixel Difference Intensity')
    
    # Row 3: RGB Histogram comparison
    ax4 = plt.subplot2grid((3, 2), (2, 0))
    ax5 = plt.subplot2grid((3, 2), (2, 1))
    
    # Plot histograms
    color = ('b', 'g', 'r')
    for i, c in enumerate(color):
        # Original image histogram
        hist1 = cv2.calcHist([img1_np], [i], None, [256], [0, 256])
        ax4.plot(hist1, color=c)
        
        # Fine-tuned image histogram
        hist2 = cv2.calcHist([img2_np], [i], None, [256], [0, 256])
        ax5.plot(hist2, color=c)
    
    ax4.set_title(f"{title1} Color Histogram", fontsize=12)
    ax4.set_xlabel('Pixel Value')
    ax4.set_ylabel('Frequency')
    ax4.legend(['Blue', 'Green', 'Red'])
    ax4.set_xlim([0, 256])
    
    ax5.set_title(f"{title2} Color Histogram", fontsize=12)
    ax5.set_xlabel('Pixel Value')
    ax5.set_ylabel('Frequency')
    ax5.legend(['Blue', 'Green', 'Red'])
    ax5.set_xlim([0, 256])
    
    # Add metrics as a text box
    metrics_text = "\n".join([
        f"SSIM: {metrics['ssim']:.4f} (higher is better)",
        f"MSE: {metrics['mse']:.2f} (lower is better)",
        f"Histogram Similarity: {metrics['hist_similarity']:.4f} (higher is better)",
        f"Perceptual Hash Similarity: {metrics['hash_similarity']:.4f} (higher is better)",
        f"Sharpness Ratio: {metrics['sharpness_ratio']:.4f} (closer to 1.0 is better)",
        f"Color Similarity: {metrics['color_similarity']:.4f} (higher is better)"
    ])
    
    plt.figtext(0.5, 0.02, metrics_text, ha="center", fontsize=10, 
                bbox={"facecolor":"white", "alpha":0.8, "pad":5, "boxstyle":"round"})
    
    plt.tight_layout(rect=[0, 0.05, 1, 0.97])
    plt.suptitle("Detailed Image Comparison", fontsize=16, y=0.98)
    
    # Save the figure if a path is provided
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
    
    return fig

# Enhanced function to generate comparison images with detailed analysis
def generate_detailed_comparisons(orig_unet_state_dict):
    """Generate comprehensive comparison images with detailed metrics for research paper"""
    print("\nüìä Generating detailed comparisons for research paper analysis...")
    
    # Create directory for comparison images
    comparison_dir = BASE_DIR / "research_comparisons"
    comparison_dir.mkdir(exist_ok=True, parents=True)
    
    # More diverse test prompts covering different scenarios
    test_prompts = {
        "standard": [
            "A golden retriever dog sitting in a park on a sunny day, highly detailed photograph",
            "A husky dog running in snow at sunset, professional photography"
        ],
        "challenging": [
            "A black labrador retriever swimming in a lake, action shot, detailed fur, splashing water",
            "A dog catching a frisbee mid-air, dynamic pose, motion blur, outdoor setting"
        ],
        "closeup": [
            "Close-up portrait of a German shepherd dog, detailed eyes and fur texture, studio lighting",
            "Macro shot of a dog's nose and whiskers, extreme detail, shallow depth of field"
        ]
    }
    
    # Make a copy of the current (fine-tuned) model state
    finetuned_pipeline = copy.deepcopy(sd_pipe)
    
    # Create a new pipeline for original model
    from diffusers import StableDiffusionPipeline
    
    # Load a fresh pipeline for comparison to avoid model state conflicts
    original_pipeline = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16,
        safety_checker=None
    ).to(device)
    
    # Apply memory optimizations
    if hasattr(original_pipeline, "enable_attention_slicing"):
        original_pipeline.enable_attention_slicing()
    
    # Create storage for metrics
    all_metrics = {}
    combined_metrics = {"original_vs_finetuned": {}}
    
    # Process each category
    for category, prompts in test_prompts.items():
        print(f"\nGenerating {category} comparisons...")
        
        # Create category directory
        category_dir = comparison_dir / category
        category_dir.mkdir(exist_ok=True)
        
        category_metrics = []
        
        # Process each prompt in the category
        for i, prompt in enumerate(prompts):
            prompt_metrics = {}
            
            # Create a clean filename from the prompt
            prompt_file = f"{category}_{i+1}"
            
            print(f"  Generating for prompt: {prompt[:50]}...")
            
            # Set seed for reproducibility
            generator = torch.Generator(device=device).manual_seed(CONFIG["seed"] + i * 100)
            
            # Generate with original model
            with torch.inference_mode():
                original_output = original_pipeline(
                    prompt,
                    num_inference_steps=50,  # More steps for quality
                    guidance_scale=7.5,
                    generator=generator,
                    height=512,
                    width=512
                )
                original_image = original_output.images[0]
                
                # Save original image
                original_path = category_dir / f"original_{prompt_file}.png"
                original_image.save(original_path)
            
            # Free memory
            del original_output
            torch.cuda.empty_cache()
            gc.collect()
            
            # Generate with fine-tuned model
            with torch.inference_mode():
                finetuned_output = finetuned_pipeline(
                    prompt,
                    num_inference_steps=50,  # Same steps for fair comparison
                    guidance_scale=7.5,
                    generator=generator,
                    height=512,
                    width=512
                )
                finetuned_image = finetuned_output.images[0]
                
                # Save fine-tuned image
                finetuned_path = category_dir / f"finetuned_{prompt_file}.png"
                finetuned_image.save(finetuned_path)
            
            # Free memory
            del finetuned_output
            torch.cuda.empty_cache()
            gc.collect()
            
            # Generate with SDXL (reference high-quality model)
            with torch.inference_mode():
                sdxl_output = sdxl_pipe(
                    prompt,
                    num_inference_steps=50,
                    guidance_scale=7.5,
                    generator=generator,
                    height=512,
                    width=512
                )
                sdxl_image = sdxl_output.images[0]
                
                # Save SDXL image
                sdxl_path = category_dir / f"sdxl_{prompt_file}.png"
                sdxl_image.save(sdxl_path)
            
            # Free memory
            del sdxl_output
            torch.cuda.empty_cache()
            gc.collect()
            
            # Calculate metrics for all comparisons
            metrics_orig_vs_finetuned = calculate_image_similarity(original_image, finetuned_image)
            metrics_orig_vs_sdxl = calculate_image_similarity(original_image, sdxl_image)
            metrics_finetuned_vs_sdxl = calculate_image_similarity(finetuned_image, sdxl_image)
            
            # Store metrics
            prompt_metrics = {
                "original_vs_finetuned": metrics_orig_vs_finetuned,
                "original_vs_sdxl": metrics_orig_vs_sdxl,
                "finetuned_vs_sdxl": metrics_finetuned_vs_sdxl
            }
            
            # Create detailed comparisons
            print("  Creating detailed visualizations...")
            
            # Original vs Fine-tuned
            fig1 = create_detailed_comparison(
                original_image, finetuned_image, 
                metrics_orig_vs_finetuned,
                "Original SD 1.5", "Fine-tuned SD 1.5",
                category_dir / f"comparison_orig_vs_finetuned_{prompt_file}.png"
            )
            plt.close(fig1)
            
            # Original vs SDXL
            fig2 = create_detailed_comparison(
                original_image, sdxl_image, 
                metrics_orig_vs_sdxl,
                "Original SD 1.5", "SDXL (Reference)",
                category_dir / f"comparison_orig_vs_sdxl_{prompt_file}.png"
            )
            plt.close(fig2)
            
            # Fine-tuned vs SDXL
            fig3 = create_detailed_comparison(
                finetuned_image, sdxl_image, 
                metrics_finetuned_vs_sdxl,
                "Fine-tuned SD 1.5", "SDXL (Reference)",
                category_dir / f"comparison_finetuned_vs_sdxl_{prompt_file}.png"
            )
            plt.close(fig3)
            
            # Create a side-by-side of all three
            comparison_all = Image.new('RGB', (512*3, 512), color='white')
            comparison_all.paste(original_image, (0, 0))
            comparison_all.paste(finetuned_image, (512, 0))
            comparison_all.paste(sdxl_image, (512*2, 0))
            
            # Add labels
            draw = ImageDraw.Draw(comparison_all)
            try:
                font = ImageFont.truetype("arial.ttf", 20)
            except:
                font = ImageFont.load_default()
                
            draw.text((10, 10), "Original SD 1.5", fill=(255, 255, 255), font=font)
            draw.text((512+10, 10), "Fine-tuned SD 1.5", fill=(255, 255, 255), font=font)
            draw.text((512*2+10, 10), "SDXL (Reference)", fill=(255, 255, 255), font=font)
            
            # Save the triple comparison
            comparison_path = category_dir / f"comparison_all_{prompt_file}.png"
            comparison_all.save(comparison_path)
            
            # Add metrics to a CSV file for the prompt
            csv_path = category_dir / f"metrics_{prompt_file}.csv"
            with open(csv_path, "w") as f:
                f.write("Comparison,SSIM,MSE,Histogram Similarity,Hash Similarity,Sharpness Ratio,Color Similarity\n")
                
                # Original vs Fine-tuned
                metrics = metrics_orig_vs_finetuned
                f.write(f"Original vs Fine-tuned,{metrics['ssim']:.6f},{metrics['mse']:.6f},{metrics['hist_similarity']:.6f},{metrics['hash_similarity']:.6f},{metrics['sharpness_ratio']:.6f},{metrics['color_similarity']:.6f}\n")
                
                # Original vs SDXL
                metrics = metrics_orig_vs_sdxl
                f.write(f"Original vs SDXL,{metrics['ssim']:.6f},{metrics['mse']:.6f},{metrics['hist_similarity']:.6f},{metrics['hash_similarity']:.6f},{metrics['sharpness_ratio']:.6f},{metrics['color_similarity']:.6f}\n")
                
                # Fine-tuned vs SDXL
                metrics = metrics_finetuned_vs_sdxl
                f.write(f"Fine-tuned vs SDXL,{metrics['ssim']:.6f},{metrics['mse']:.6f},{metrics['hist_similarity']:.6f},{metrics['hash_similarity']:.6f},{metrics['sharpness_ratio']:.6f},{metrics['color_similarity']:.6f}\n")
            
            # Save full metrics data as JSON
            json_path = category_dir / f"metrics_{prompt_file}.json"
            with open(json_path, "w") as f:
                json.dump(prompt_metrics, f, indent=2)
            
            # Accumulate metrics for category
            category_metrics.append(prompt_metrics)
            
            print(f"  ‚úì Completed analysis for prompt {i+1}/{len(prompts)}")
        
        # Calculate average metrics for the category
        category_avg = {"original_vs_finetuned": {}, "original_vs_sdxl": {}, "finetuned_vs_sdxl": {}}
        
        for comparison in category_avg.keys():
            for metric in ["ssim", "mse", "hist_similarity", "hash_similarity", "sharpness_ratio", "color_similarity"]:
                values = [item[comparison][metric] for item in category_metrics]
                category_avg[comparison][metric] = sum(values) / len(values)
        
        # Save category average metrics
        with open(category_dir / "category_averages.json", "w") as f:
            json.dump(category_avg, f, indent=2)
        
        # Add to overall metrics
        all_metrics[category] = category_avg
        
        # Accumulate to combined metrics
        for metric in ["ssim", "mse", "hist_similarity", "hash_similarity", "sharpness_ratio", "color_similarity"]:
            if metric not in combined_metrics["original_vs_finetuned"]:
                combined_metrics["original_vs_finetuned"][metric] = []
            
            combined_metrics["original_vs_finetuned"][metric].append(
                category_avg["original_vs_finetuned"][metric]
            )
        
        print(f"‚úì Completed {category} category analysis")
    
    # Calculate overall averages
    overall_avg = {"original_vs_finetuned": {}, "original_vs_sdxl": {}, "finetuned_vs_sdxl": {}}
    
    for comparison in overall_avg.keys():
        for metric in ["ssim", "mse", "hist_similarity", "hash_similarity", "sharpness_ratio", "color_similarity"]:
            values = []
            for category in all_metrics.keys():
                values.append(all_metrics[category][comparison][metric])
            overall_avg[comparison][metric] = sum(values) / len(values)
    
    # Save overall metrics
    with open(comparison_dir / "overall_metrics.json", "w") as f:
        json.dump(overall_avg, f, indent=2)
    
    # Create summary graphs for paper
    create_metrics_summary_graphs(all_metrics, comparison_dir)
    
    print(f"‚úÖ Detailed comparison analysis completed. Results saved to {comparison_dir}")
    
    return all_metrics

# Create summary graphs for the paper
def create_metrics_summary_graphs(all_metrics, output_dir):
    """Create summary graphs for metrics across categories"""
    print("\nüìä Creating summary graphs for paper...")
    
    # Extract categories
    categories = list(all_metrics.keys())
    
    # Create bar charts for each metric
    metrics_to_plot = [
        ("ssim", "Structural Similarity (SSIM)", "higher is better"),
        ("mse", "Mean Squared Error (MSE)", "lower is better"),
        ("hist_similarity", "Histogram Similarity", "higher is better"),
        ("hash_similarity", "Perceptual Hash Similarity", "higher is better"),
        ("color_similarity", "Color Similarity", "higher is better")
    ]
    
    for metric_key, metric_name, metric_desc in metrics_to_plot:
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Get values for each category
        orig_vs_ft_values = [all_metrics[cat]["original_vs_finetuned"][metric_key] for cat in categories]
        orig_vs_sdxl_values = [all_metrics[cat]["original_vs_sdxl"][metric_key] for cat in categories]
        ft_vs_sdxl_values = [all_metrics[cat]["finetuned_vs_sdxl"][metric_key] for cat in categories]
        
        # Set up bar positions
        x = np.arange(len(categories))
        width = 0.25
        
        # Create bars
        ax.bar(x - width, orig_vs_ft_values, width, label="Original vs. Fine-tuned", color='#3366CC')
        ax.bar(x, orig_vs_sdxl_values, width, label="Original vs. SDXL", color='#DC3912')
        ax.bar(x + width, ft_vs_sdxl_values, width, label="Fine-tuned vs. SDXL", color='#109618')
        
        # Add labels and legend
        ax.set_xlabel('Image Category')
        ax.set_ylabel(metric_name)
        ax.set_title(f'{metric_name} by Category ({metric_desc})')
        ax.set_xticks(x)
        ax.set_xticklabels(categories)
        ax.legend()
        
        # Add value labels on bars
        for i, v in enumerate(orig_vs_ft_values):
            ax.text(i - width, v + (0.01 if metric_key != "mse" else v*0.05), 
                   f"{v:.3f}", ha='center', va='bottom', fontsize=8, rotation=90)
        
        for i, v in enumerate(orig_vs_sdxl_values):
            ax.text(i, v + (0.01 if metric_key != "mse" else v*0.05), 
                   f"{v:.3f}", ha='center', va='bottom', fontsize=8, rotation=90)
        
        for i, v in enumerate(ft_vs_sdxl_values):
            ax.text(i + width, v + (0.01 if metric_key != "mse" else v*0.05), 
                   f"{v:.3f}", ha='center', va='bottom', fontsize=8, rotation=90)
        
        # Save figure
        plt.tight_layout()
        plt.savefig(output_dir / f"{metric_key}_comparison.png", dpi=300)
        plt.close()
    
    # Create radar chart for overall metrics
    create_overall_radar_chart(all_metrics, output_dir)
    
    # Create improvement percentage chart
    create_improvement_chart(all_metrics, output_dir)
    
    print("‚úì Created summary graphs for research paper")

# Create radar chart showing overall metrics
def create_overall_radar_chart(all_metrics, output_dir):
    """Create a radar chart showing overall metrics for the paper"""
    # Calculate overall averages for each comparison
    overall = {"original_vs_finetuned": {}, "original_vs_sdxl": {}, "finetuned_vs_sdxl": {}}
    
    metrics_to_include = ["ssim", "hist_similarity", "hash_similarity", "color_similarity"]
    
    # Average across all categories
    categories = list(all_metrics.keys())
    
    for comparison in overall.keys():
        for metric in metrics_to_include:
            values = [all_metrics[cat][comparison][metric] for cat in categories]
            overall[comparison][metric] = sum(values) / len(values)
        
        # Add inverse MSE (normalized to 0-1, higher is better)
        mse_values = [all_metrics[cat][comparison]["mse"] for cat in categories]
        avg_mse = sum(mse_values) / len(mse_values)
        # Normalize: 1 = perfect (MSE=0), 0 = worst (MSE=10000 or more)
        overall[comparison]["mse_inv"] = max(0, 1 - min(avg_mse / 10000, 1))
    
    # Create radar chart
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, polar=True)
    
    # Labels for the chart
    labels = ["SSIM", "Histogram\nSimilarity", "Perceptual\nSimilarity", "Color\nSimilarity", "MSE\n(inverted)"]
    
    # Get values for each comparison
    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()
    angles += angles[:1]  # Close the loop
    
    # Add MSE (inverted) to metrics_to_include for the actual data extraction
    metrics_order = metrics_to_include + ["mse_inv"]
    
    # Set up legend labels
    legend_labels = {
        "original_vs_finetuned": "Original vs. Fine-tuned",
        "original_vs_sdxl": "Original vs. SDXL",
        "finetuned_vs_sdxl": "Fine-tuned vs. SDXL"
    }
    
    # Colors for each comparison
    colors = {
        "original_vs_finetuned": '#3366CC',
        "original_vs_sdxl": '#DC3912',
        "finetuned_vs_sdxl": '#109618'
    }
    
    # Plot each comparison
    for comparison, color in colors.items():
        values = [overall[comparison][m] for m in metrics_order]
        values += values[:1]  # Close the loop
        
        ax.plot(angles, values, 'o-', linewidth=2, color=color, label=legend_labels[comparison])
        ax.fill(angles, values, alpha=0.1, color=color)
    
    # Set chart properties
    ax.set_thetagrids(np.degrees(angles[:-1]), labels)
    ax.set_ylim(0, 1)
    ax.grid(True)
    
    # Add legend
    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    
    plt.title("Overall Image Quality Metrics\n(higher values are better for all metrics)", size=15)
    plt.tight_layout()
    
    # Save figure
    plt.savefig(output_dir / "overall_radar_chart.png", dpi=300)
    plt.close()

# Create improvement percentage chart
def create_improvement_chart(all_metrics, output_dir):
    """Create a chart showing percentage improvements from original to fine-tuned model"""
    # Calculate improvements
    improvements = {}
    categories = list(all_metrics.keys())
    
    metrics_to_analyze = {
        "ssim": "higher",
        "mse": "lower",
        "hist_similarity": "higher", 
        "hash_similarity": "higher",
        "color_similarity": "higher"
    }
    
    # For each category, calculate percentage improvement
    for category in categories:
        improvements[category] = {}
        
        for metric, better in metrics_to_analyze.items():
            orig_vs_sdxl = all_metrics[category]["original_vs_sdxl"][metric]
            ft_vs_sdxl = all_metrics[category]["finetuned_vs_sdxl"][metric]
            
            # Calculate how much closer fine-tuned is to SDXL than original is
            if better == "higher":
                # For metrics where higher is better
                if orig_vs_sdxl > 0:
                    improvement = ((ft_vs_sdxl - orig_vs_sdxl) / orig_vs_sdxl) * 100
                else:
                    improvement = 100 if ft_vs_sdxl > 0 else 0
            else:
                # For metrics where lower is better (only MSE)
                if orig_vs_sdxl > 0:
                    improvement = ((orig_vs_sdxl - ft_vs_sdxl) / orig_vs_sdxl) * 100
                else:
                    improvement = 100 if ft_vs_sdxl < orig_vs_sdxl else 0
            
            # Store improvement
            improvements[category][metric] = improvement
    
    # Create a summary chart
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Set up bar positions
    x = np.arange(len(metrics_to_analyze))
    width = 0.8 / len(categories)
    
    # Colors for each category
    colors = ['#3366CC', '#DC3912', '#FF9900', '#109618', '#990099']
    
    # Plot improvements for each category
    for i, category in enumerate(categories):
        category_improvements = [improvements[category][metric] for metric in metrics_to_analyze.keys()]
        position = x - 0.4 + (i + 0.5) * width
        bars = ax.bar(position, category_improvements, width, label=category, color=colors[i % len(colors)])
        
        # Add value labels
        for bar, value in zip(bars, category_improvements):
            height = bar.get_height()
            label_height = max(height, 0)  # Handle negative improvements
            va = 'bottom' if height >= 0 else 'top'
            ax.text(bar.get_x() + bar.get_width()/2, label_height,
                   f'{value:.1f}%', ha='center', va=va, fontsize=8, rotation=0)
    
    # Add labels and legend
    ax.set_ylabel('Improvement Percentage (%)')
    ax.set_title('Improvement from Original to Fine-tuned Model vs. SDXL Reference')
    ax.set_xticks(x)
    ax.set_xticklabels(list(metrics_to_analyze.keys()))
    ax.legend()
    
    # Add a horizontal line at y=0
    ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)
    
    # Add a note about the metrics
    plt.figtext(0.5, 0.01, 
               "Note: For all metrics, higher percentages indicate greater improvement toward SDXL quality.",
               ha="center", fontsize=10, bbox={"facecolor":"white", "alpha":0.8, "pad":5})
    
    # Save figure
    plt.tight_layout(rect=[0, 0.05, 1, 0.97])
    plt.savefig(output_dir / "improvement_percentage_chart.png", dpi=300)
    plt.close()
    
    # Also create a table of improvements
    fig, ax = plt.subplots(figsize=(12, len(categories) + 2))
    ax.axis('tight')
    ax.axis('off')
    
    # Prepare table data
    table_data = []
    for category in categories:
        row = [category]
        for metric in metrics_to_analyze:
            row.append(f"{improvements[category][metric]:.2f}%")
        table_data.append(row)
    
    # Create the table
    table = ax.table(
        cellText=table_data,
        colLabels=['Category'] + list(metrics_to_analyze.keys()),
        loc='center',
        cellLoc='center'
    )
    
    # Style the table
    table.auto_set_font_size(False)
    table.set_fontsize(12)
    table.scale(1, 1.5)
    
    # Color positive and negative values
    for i in range(len(categories)):
        for j, metric in enumerate(metrics_to_analyze.keys()):
            cell = table[i+1, j+1]
            improvement_val = improvements[categories[i]][metric]
            
            if improvement_val > 5:
                cell.set_facecolor('#d8f3dc')  # Light green for good improvement
            elif improvement_val > 0:
                cell.set_facecolor('#e9f5db')  # Very light green for small improvement
            elif improvement_val < -5:
                cell.set_facecolor('#ffccd5')  # Light red for significant regression
            elif improvement_val < 0:
                cell.set_facecolor('#ffe5ec')  # Very light red for small regression
    
    plt.title("Percentage Improvement by Category and Metric", fontsize=14)
    plt.tight_layout()
    plt.savefig(output_dir / "improvement_table.png", dpi=300)
    plt.close()
    
    return improvements

# Main function
def main():
    """Main function to generate high-quality dog images and fine-tune model"""
    print("\nüöÄ Starting high-quality image generation and fine-tuning experiment")
    
    start_time = time.time()
    
    try:
        # Generate reference images from SDXL
        reference_images, detailed_prompts = generate_reference_images()
        
        if len(reference_images) == 0:
            print("‚ùå Failed to generate any reference images. Exiting.")
            return
        
        # Fine-tune the weak model with LoRA
        orig_unet_state_dict = setup_and_train_lora(reference_images, detailed_prompts)
        
        # Generate comparison images
        generate_comparison_images(orig_unet_state_dict)
        
        # Generate detailed metrics and visualizations for research paper
        metrics_data = generate_detailed_comparisons(orig_unet_state_dict)
        
        # Calculate training time
        training_time = time.time() - start_time
        hours, remainder = divmod(training_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        
        print(f"\n‚è±Ô∏è Training and analysis completed in {int(hours)}h {int(minutes)}m {int(seconds)}s")
        print("\n‚úÖ Process completed successfully!")
        print(f"üñºÔ∏è Check the generated images and metrics in: {BASE_DIR}/reference_images and {BASE_DIR}/research_comparisons")
        
        # Create a summary report
        create_research_paper_summary(metrics_data, training_time)
        
    except Exception as e:
        print(f"\n‚ùå Error during execution: {e}")
        import traceback
        traceback.print_exc()
        
        print("\nüîÑ Generating standard dog images without training...")
        generate_standard_dog_images()

# Function to generate standard dog images without training
def generate_standard_dog_images():
    """Generate standard dog images with both models if training fails"""
    # Create output directory
    output_dir = BASE_DIR / "standard_outputs"
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Test prompts
    test_prompts = [
        "A golden retriever dog sitting in a park on a sunny day, highly detailed photograph",
        "A husky dog running in snow at sunset, professional photography"
    ]
    
    # Generate with SD 1.5
    for i, prompt in enumerate(test_prompts):
        try:
            # Set seed for reproducibility
            generator = torch.Generator(device=device).manual_seed(42 + i)
            
            # Generate image
            with torch.inference_mode():
                image = sd_pipe(
                    prompt,
                    num_inference_steps=30,
                    guidance_scale=7.5,
                    generator=generator
                ).images[0]
                
                # Save image
                image_path = output_dir / f"sd15_dog_{i+1}.png"
                image.save(image_path)
                print(f"‚úì Saved SD 1.5 image {i+1}")
            
            # Free memory
            torch.cuda.empty_cache()
            gc.collect()
            
        except Exception as e:
            print(f"Error generating SD 1.5 image: {e}")
    
    # Generate with SDXL if available
    if 'sdxl_pipe' in globals():
        for i, prompt in enumerate(test_prompts):
            try:
                # Set seed for reproducibility
                generator = torch.Generator(device=device).manual_seed(42 + i)
                
                # Generate image
                with torch.inference_mode():
                    image = sdxl_pipe(
                        prompt,
                        num_inference_steps=30,
                        guidance_scale=7.5,
                        generator=generator
                    ).images[0]
                    
                    # Save image
                    image_path = output_dir / f"sdxl_dog_{i+1}.png"
                    image.save(image_path)
                    print(f"‚úì Saved SDXL image {i+1}")
                
                # Free memory
                torch.cuda.empty_cache()
                gc.collect()
                
            except Exception as e:
                print(f"Error generating SDXL image: {e}")
    
    print(f"‚úì Standard images saved to {output_dir}")

def create_research_paper_summary(metrics_data, training_time):
    """Create a summary report with key findings for the research paper"""
    print("\nüìÑ Creating summary report for research paper...")
    
    report_dir = BASE_DIR / "research_paper_materials"
    report_dir.mkdir(exist_ok=True)
    
    # Calculate hours and minutes from training time
    hours, remainder = divmod(training_time, 3600)
    minutes, seconds = divmod(remainder, 60)
    
    # Create a markdown report
    report_path = report_dir / "research_summary.md"
    
    with open(report_path, "w") as f:
        f.write("# Knowledge Transfer in Image Generation Models: Research Summary\n\n")
        f.write(f"*Generated on {datetime.now().strftime('%Y-%m-%d %H:%M')}*\n\n")
        
        f.write("## Experiment Configuration\n\n")
        f.write("| Parameter | Value |\n")
        f.write("|-----------|-------|\n")
        f.write(f"| Base Model | Stable Diffusion 1.5 |\n")
        f.write(f"| Teacher Model | Stable Diffusion XL |\n")
        f.write(f"| Fine-tuning Method | LoRA (Low-Rank Adaptation) |\n")
        f.write(f"| LoRA Rank | {CONFIG['lora_r']} |\n")
        f.write(f"| LoRA Alpha | {CONFIG['lora_alpha']} |\n")
        f.write(f"| Image Size | {CONFIG['image_size']}x{CONFIG['image_size']} |\n")
        f.write(f"| Inference Steps | {CONFIG['inference_steps']} |\n")
        f.write(f"| Training Time | {int(hours)}h {int(minutes)}m |\n\n")
        
        f.write("## Key Findings\n\n")
        
        # Calculate overall improvements across all categories
        categories = list(metrics_data.keys())
        overall_improvement = {}
        
        metrics_to_analyze = {
            "ssim": "higher",
            "mse": "lower",
            "hist_similarity": "higher", 
            "hash_similarity": "higher",
            "color_similarity": "higher"
        }
        
        for metric, better in metrics_to_analyze.items():
            improvements = []
            for category in categories:
                orig_vs_sdxl = metrics_data[category]["original_vs_sdxl"][metric]
                ft_vs_sdxl = metrics_data[category]["finetuned_vs_sdxl"][metric]
                
                # Calculate relative improvement
                if better == "higher":
                    # For metrics where higher is better
                    if orig_vs_sdxl > 0:
                        improvement = ((ft_vs_sdxl - orig_vs_sdxl) / orig_vs_sdxl) * 100
                    else:
                        improvement = 100 if ft_vs_sdxl > 0 else 0
                else:
                    # For metrics where lower is better (only MSE)
                    if orig_vs_sdxl > 0:
                        improvement = ((orig_vs_sdxl - ft_vs_sdxl) / orig_vs_sdxl) * 100
                    else:
                        improvement = 100 if ft_vs_sdxl < orig_vs_sdxl else 0
                
                improvements.append(improvement)
            
            # Average improvement across categories
            overall_improvement[metric] = sum(improvements) / len(improvements)
        
        # Write overall findings
        f.write("### Overall Improvements\n\n")
        f.write("After fine-tuning the Stable Diffusion 1.5 model using LoRA with images generated by SDXL as reference, the following improvements were observed:\n\n")
        
        f.write("| Metric | Improvement |\n")
        f.write("|--------|------------|\n")
        for metric, improvement in overall_improvement.items():
            metric_name = {
                "ssim": "Structural Similarity (SSIM)",
                "mse": "Mean Squared Error (MSE)",
                "hist_similarity": "Histogram Similarity",
                "hash_similarity": "Perceptual Hash Similarity",
                "color_similarity": "Color Distribution Similarity"
            }[metric]
            
            direction = "increase" if improvement > 0 else "decrease"
            f.write(f"| {metric_name} | {abs(improvement):.2f}% {direction} |\n")
        
        f.write("\n### Category-Specific Findings\n\n")
        
        # Write category-specific findings
        for category in categories:
            f.write(f"#### {category.title()} Images\n\n")
            
            category_metrics = metrics_data[category]
            
            # Compute improvements for this category
            improvements = {}
            for metric, better in metrics_to_analyze.items():
                orig_vs_sdxl = category_metrics["original_vs_sdxl"][metric]
                ft_vs_sdxl = category_metrics["finetuned_vs_sdxl"][metric]
                
                # Calculate relative improvement
                if better == "higher":
                    # For metrics where higher is better
                    if orig_vs_sdxl > 0:
                        improvement = ((ft_vs_sdxl - orig_vs_sdxl) / orig_vs_sdxl) * 100
                    else:
                        improvement = 100 if ft_vs_sdxl > 0 else 0
                else:
                    # For metrics where lower is better (only MSE)
                    if orig_vs_sdxl > 0:
                        improvement = ((orig_vs_sdxl - ft_vs_sdxl) / orig_vs_sdxl) * 100
                    else:
                        improvement = 100 if ft_vs_sdxl < orig_vs_sdxl else 0
                
                improvements[metric] = improvement
            
            # Determine which metrics showed the most improvement
            sorted_improvements = sorted(improvements.items(), key=lambda x: abs(x[1]), reverse=True)
            best_metrics = [m for m, v in sorted_improvements[:2]]
            
            # Write findings for this category
            if all(improvements[m] > 0 for m in best_metrics):
                f.write(f"The fine-tuned model showed significant improvements in {best_metrics[0]} ({improvements[best_metrics[0]]:.1f}%) and {best_metrics[1]} ({improvements[best_metrics[1]]:.1f}%) compared to the original model.\n\n")
            elif all(improvements[m] < 0 for m in best_metrics):
                f.write(f"The fine-tuned model showed decreased performance in {best_metrics[0]} ({abs(improvements[best_metrics[0]]):.1f}%) and {best_metrics[1]} ({abs(improvements[best_metrics[1]]):.1f}%) compared to the original model.\n\n")
            else:
                f.write(f"The fine-tuned model showed mixed results with improvements in some metrics and decreased performance in others.\n\n")
            
            # Add image references
            f.write(f"![{category.title()} Comparison](../research_comparisons/{category}/comparison_all_{category}_1.png)\n\n")
        
        f.write("## Conclusion\n\n")
        
        # Determine overall conclusion based on average improvement
        avg_improvement = sum(overall_improvement.values()) / len(overall_improvement)
        
        if avg_improvement > 10:
            f.write("The experiments demonstrate that knowledge transfer from a strong model (SDXL) to a weaker model (SD 1.5) using LoRA fine-tuning is highly effective. The fine-tuned model produces images that are significantly closer to SDXL quality while maintaining the computational efficiency of the smaller model.\n\n")
        elif avg_improvement > 0:
            f.write("The experiments show moderate improvements through knowledge transfer from SDXL to SD 1.5 using LoRA fine-tuning. While not dramatic, the consistent improvements across multiple metrics indicate that the approach has merit and could be further optimized.\n\n")
        else:
            f.write("The experiments show mixed results for knowledge transfer from SDXL to SD 1.5 using LoRA fine-tuning. While some specific cases showed improvement, the overall performance suggests that alternative approaches or hyperparameter configurations should be explored.\n\n")
        
        f.write("### Future Work\n\n")
        f.write("1. Explore different LoRA configurations (rank, alpha, target modules)\n")
        f.write("2. Test with a wider variety of image types and prompts\n")
        f.write("3. Investigate the impact of training duration on improvement metrics\n")
        f.write("4. Compare with other fine-tuning methods beyond LoRA\n")
        f.write("5. Conduct user studies to evaluate perceived image quality improvements\n")
    
    print(f"‚úÖ Research summary created at {report_path}")
    
    # Copy key visualizations to the research materials folder
    key_files = [
        "research_comparisons/overall_radar_chart.png",
        "research_comparisons/improvement_percentage_chart.png",
        "research_comparisons/ssim_comparison.png",
        "research_comparisons/mse_comparison.png"
    ]
    
    for file_path in key_files:
        src = BASE_DIR / file_path
        if src.exists():
            dst = report_dir / src.name
            shutil.copy(src, dst)
    
    return report_path

if __name__ == "__main__":
    main()